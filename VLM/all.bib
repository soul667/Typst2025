@misc{250819236MemoryVLAPerceptualCognitive,
  title = {[2508.19236] {{MemoryVLA}}: {{Perceptual-Cognitive Memory}} in {{Vision-Language-Action Models}} for {{Robotic Manipulation}}},
  urldate = {2025-09-16},
  howpublished = {https://arxiv.org/abs/2508.19236},
  file = {C:\Users\guaoxiang\Zotero\storage\HXDKVTF6\2508.html}
}

@incollection{abuzainaSphereDetectionKinect2013,
  title = {Sphere {{Detection}} in {{Kinect Point Clouds}} via the {{3D Hough Transform}}},
  booktitle = {Computer {{Analysis}} of {{Images}} and {{Patterns}}},
  author = {Abuzaina, Anas and Nixon, Mark S. and Carter, John N.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Wilson, Richard and Hancock, Edwin and Bors, Adrian and Smith, William},
  year = {2013},
  volume = {8048},
  pages = {290--297},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40246-3_36},
  urldate = {2024-11-22},
  abstract = {We introduce a fast, robust and accurate Hough Transform (HT) based algorithm for detecting spherical structures in 3D point clouds. To our knowledge, our algorithm is the first HT based implementation that detects spherical structures in typical in 3D point clouds generated by consumer depth sensors such as the Microsoft Kinect. Our approach has been designed to be computationally efficient; reducing an established limitation of HT based approaches. We provide experimental analysis of the achieved results, showing a robust performance against occlusion, and we show superior performance to the only other HT based algorithm for detecting spheres in point clouds available in literature.},
  isbn = {978-3-642-40245-6 978-3-642-40246-3},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\99TX74JX\Abuzaina 等 - 2013 - Sphere Detection in Kinect Point Clouds via the 3D Hough Transform.pdf}
}

@misc{adamkiewiczVisionOnlyRobotNavigation2022,
  title = {Vision-{{Only Robot Navigation}} in a {{Neural Radiance World}}},
  author = {Adamkiewicz, Michal and Chen, Timothy and Caccavale, Adam and Gardner, Rachel and Culbertson, Preston and Bohg, Jeannette and Schwager, Mac},
  year = {2022},
  month = jan,
  number = {arXiv:2110.00168},
  eprint = {2110.00168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.00168},
  urldate = {2025-08-11},
  abstract = {Neural Radiance Fields (NeRFs) have recently emerged as a powerful paradigm for the representation of natural, complex 3D scenes. NeRFs represent continuous volumetric density and RGB values in a neural network, and generate photo-realistic images from unseen camera viewpoints through ray tracing. We propose an algorithm for navigating a robot through a 3D environment represented as a NeRF using only an on-board RGB camera for localization. We assume the NeRF for the scene has been pre-trained offline, and the robot's objective is to navigate through unoccupied space in the NeRF to reach a goal pose. We introduce a trajectory optimization algorithm that avoids collisions with high-density regions in the NeRF based on a discrete time version of differential flatness that is amenable to constraining the robot's full pose and control inputs. We also introduce an optimization based filtering method to estimate 6DoF pose and velocities for the robot in the NeRF given only an onboard RGB camera. We combine the trajectory planner with the pose filter in an online replanning loop to give a vision-based robot navigation pipeline. We present simulation results with a quadrotor robot navigating through a jungle gym environment, the inside of a church, and Stonehenge using only an RGB camera. We also demonstrate an omnidirectional ground robot navigating through the church, requiring it to reorient to fit through the narrow gap. Videos of this work can be found at https://mikh3x4.github.io/nerf-navigation/ .},
  archiveprefix = {arXiv},
  keywords = {,Computer Science - Robotics,nerf},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\LP7BMPR4\\Adamkiewicz 等 - 2022 - Vision-Only Robot Navigation in a Neural Radiance World.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\IMWGMXQ2\\2110.html}
}

@article{akritasApplicationsSingularvalueDecomposition2004,
  title = {Applications of Singular-Value Decomposition ({{SVD}})},
  author = {Akritas, Alkiviadis G. and Malaschonok, Gennadi I.},
  year = {2004},
  month = sep,
  journal = {Mathematics and Computers in Simulation},
  series = {Applications of {{Computer Algebra}} in {{Science}}, {{Engineering}}, {{Simulation}} and {{Special Software}}},
  volume = {67},
  number = {1},
  pages = {15--31},
  issn = {0378-4754},
  doi = {10.1016/j.matcom.2004.05.005},
  urldate = {2024-11-15},
  abstract = {Let A be an m{\texttimes}n matrix with m{$\geq$}n. Then one form of the singular-value decomposition of A is A=UT{$\Sigma$}V,where U and V are orthogonal and {$\Sigma$} is square diagonal. That is, UUT=Irank(A), VVT=Irank(A), U is rank(A){\texttimes}m, V is rank(A){\texttimes}n and {$\Sigma$}={$\sigma$}10{$\cdots$}000{$\sigma$}2{$\cdots$}00{$\vdots\vdots\ddots\vdots\vdots$}00{$\cdots\sigma$}rank(A)-1000{$\cdots$}0{$\sigma$}rank(A)is a rank(A){\texttimes}rank(A) diagonal matrix. In addition {$\sigma$}1{$\geq\sigma$}2{$\geq\cdots\geq\sigma$}rank(A){$>$}0. The {$\sigma$}i's are called the singular values of A and their number is equal to the rank of A. The ratio {$\sigma$}1/{$\sigma$}rank(A) can be regarded as a condition number of the matrix A. It is easily verified that the singular-value decomposition can be also written as A=UT{$\Sigma$}V={$\sum$}i=1rank(A){$\sigma$}iuiTvi.The matrix uiTvi is the outer product of the i-th row of U with the corresponding row of V. Note that each of these matrices can be stored using only m+n locations rather than mn locations. Using both forms presented above---and following Jerry Uhl's beautiful approach in the Calculus and Mathematica book series [Matrices, Geometry \& Mathematica, Math Everywhere Inc., 1999]---we show how SVD can be used as a tool for teaching Linear Algebra geometrically, and then apply it in solving least-squares problems and in data compression. In this paper we used the Computer Algebra system Mathematica to present a purely numerical problem. In general, the use of Computer Algebra systems has greatly influenced the teaching of mathematics, allowing students to concentrate on the main ideas and to visualize them.},
  langid = {american},
  keywords = {,Applications,Singular-value decompositions,svd},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\VZJ4WETQ\\Akritas和Malaschonok - 2004 - Applications of singular-value decomposition (SVD).pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LYFHZUA3\\S037847540400151X.html}
}

@article{al-sharadqahErrorAnalysisCircle2009,
  title = {Error Analysis for Circle Fitting Algorithms},
  author = {{Al-Sharadqah}, Ali and Chernov, Nikolai},
  year = {2009},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {3},
  pages = {886--911},
  issn = {1935-7524},
  doi = {10.1214/09-EJS419},
  urldate = {2025-01-05},
  abstract = {We study the problem of fitting circles (or circular arcs) to data points observed with errors in both variables. A detailed error analysis for all popular circle fitting methods -- geometric fit, Ka{\r{}}sa fit, Pratt fit, and Taubin fit -- is presented. Our error analysis goes deeper than the traditional expansion to the leading order. We obtain higher order terms, which show exactly why and by how much circle fits differ from each other. Our analysis allows us to construct a new algebraic (non-iterative) circle fitting algorithm that outperforms all the existing methods, including the (previously regarded as unbeatable) geometric fit.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\CJG5TCBY\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.dual.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\CV288TGG\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.dual.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\GNEK275A\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\IVDQINGK\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LG58ULX7\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LS5SG9XH\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.no_watermark.zh-CN.dual.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\W8H2GGW3\\Al-Sharadqah和Chernov - 2009 - Error analysis for circle fitting algorithms.pdf}
}

@article{al-sharadqahErrorAnalysisCircle2009a,
  title = {{Error analysis for circle fitting algorithms}},
  author = {{Al-Sharadqah}, Ali and Chernov, Nikolai},
  year = {2009},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {3},
  number = {none},
  issn = {1935-7524},
  doi = {10.1214/09-EJS419},
  urldate = {2025-04-22},
  langid = {chinese}
}

@misc{ApplicationsSingularvalueDecomposition,
  title = {Applications of Singular-Value Decomposition ({{SVD}}) - {{ScienceDirect}}},
  urldate = {2024-11-15},
  howpublished = {https://www.sciencedirect.com/science/article/abs/pii/S037847540400151X},
  keywords = {,svd},
  file = {C:\Users\guaoxiang\Zotero\storage\WQ723ZBE\S037847540400151X.html}
}

@article{austinStructuredDenoisingDiffusion,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  author = {Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel},
  abstract = {Denoising diffusion probabilistic models (DDPMs) [17] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusionlike generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [18], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\3PE9IV2Y\Austin 等 - Structured Denoising Diffusion Models in Discrete State-Spaces.pdf}
}

@incollection{bederDirectSolutionsComputing2006,
  title = {Direct {{Solutions}} for {{Computing Cylinders}} from {{Minimal Sets}} of {{3D Points}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2006},
  author = {Beder, Christian and F{\"o}rstner, Wolfgang},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  volume = {3951},
  pages = {135--146},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11744023_11},
  urldate = {2025-01-04},
  abstract = {Efficient direct solutions for the determination of a cylinder from points are presented. The solutions range from the well known direct solution of a quadric to the minimal solution of a cylinder with five points. In contrast to the approach of G. Roth and M. D. Levine (1990), who used polynomial bases for representing the geometric entities, we use algebraic constraints on the quadric representing the cylinder. The solutions for six to eight points directly determine all the cylinder parameters in one step: (1) The eight-point-solution, similar to the estimation of the fundamental matrix, requires to solve for the roots of a 3rd-order-polynomial. (2) The seven-point-solution, similar to the sixpoint-solution for the relative orientation by J. Philip (1996), yields a linear equation system. (3) The six-point-solution, similar to the fivepoint-solution for the relative orientation by D. Nister (2003), yields a ten-by-ten eigenvalue problem. The new minimal five-point-solution first determines the direction and then the position and the radius of the cylinder. The search for the zeros of the resulting 6th order polynomials is efficiently realized using 2D-Bernstein polynomials. Also direct solutions for the special cases with the axes of the cylinder parallel to a coordinate plane or axis are given. The method is used to find cylinders in range data of an industrial site.},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-33832-1 978-3-540-33833-8},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\2S2HKMWH\Beder和Förstner - 2006 - Direct Solutions for Computing Cylinders from Minimal Sets of 3D Points.pdf}
}

@article{behley3DLiDARbasedSemantic2021,
  title = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences: {{The SemanticKITTI Dataset}}},
  shorttitle = {Towards {{3D LiDAR-based}} Semantic Scene Understanding of {{3D}} Point Cloud Sequences},
  author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Gall, J{\"u}rgen and Stachniss, Cyrill},
  year = {2021},
  month = aug,
  journal = {The International Journal of Robotics Research},
  volume = {40},
  number = {8-9},
  pages = {959--967},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/02783649211006735},
  urldate = {2024-10-13},
  abstract = {A holistic semantic scene understanding exploiting all available sensor modalities is a core capability to master selfdriving in complex everyday traffic. To this end, we present the SemanticKITTI dataset that provides point-wise semantic annotations of Velodyne HDL-64E point clouds of the KITTI Odometry Benchmark. Together with the data, we also published three benchmark tasks for semantic scene understanding covering different aspects of semantic scene understanding: (1) semantic segmentation for point-wise classification using single or multiple point clouds as input, (2) semantic scene completion for predictive reasoning on the semantics and occluded regions, and (3) panoptic segmentation combining point-wise classification and assigning individual instance identities to separate objects of the same class. In this article, we provide details on our dataset showing an unprecedented number of fully annotated point cloud sequences, more information on our labeling process to efficiently annotate such a vast amount of point clouds, and lessons learned in this process. The dataset and resources are available at http://www.semantic-kitti.org.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\JPV7MRAC\Behley 等 - 2021 - Towards 3D LiDAR-based semantic scene understanding of 3D point cloud sequences The SemanticKITTI D.pdf}
}

@misc{bermanMissionGPTMissionPlanner2024,
  title = {{{MissionGPT}}: {{Mission Planner}} for {{Mobile Robot}} Based on {{Robotics Transformer Model}}},
  shorttitle = {{{MissionGPT}}},
  author = {Berman, Vladimir and Bazhenov, Artem and Tsetserukou, Dzmitry},
  year = {2024},
  month = nov,
  number = {arXiv:2411.05107},
  eprint = {2411.05107},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.05107},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach to building mission planners based on neural networks with Transformer architecture and Large Language Models (LLMs). This approach demonstrates the possibility of setting a task for a mobile robot and its successful execution without the use of perception algorithms, based only on the data coming from the camera. In this work, a success rate of more than 50\% was obtained for one of the basic actions for mobile robots. The proposed approach is of practical importance in the field of warehouse logistics robots, as in the future it may allow to eliminate the use of markings, LiDARs, beacons and other tools for robot orientation in space. In conclusion, this approach can be scaled for any type of robot and for any number of robots.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\CWD9VXTW\\2411.05107v1-zh.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LJFK6KME\\Berman 等 - 2024 - MissionGPT Mission Planner for Mobile Robot based on Robotics Transformer Model.pdf}
}

@article{blackP0VisionLanguageActionFlow,
  title = {{$\Pi$}0: {{A Vision-Language-Action Flow Model}} for {{General Robot Control}}},
  author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and {Li-Bell}, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\GCDSK7RH\\Black 等 - π0 A Vision-Language-Action Flow Model for General Robot Control-zh.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\VGNCLN49\\Black 等 - π0 A Vision-Language-Action Flow Model for General Robot Control.pdf}
}

@misc{bochkovskiiDepthProSharp2024,
  title = {Depth {{Pro}}: {{Sharp Monocular Metric Depth}} in {{Less Than}} a {{Second}}},
  shorttitle = {Depth {{Pro}}},
  author = {Bochkovskii, Aleksei and Delaunoy, Ama{\"e}l and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  year = {2024},
  month = oct,
  number = {arXiv:2410.02073},
  eprint = {2410.02073},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02073},
  urldate = {2024-10-21},
  abstract = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  archiveprefix = {arXiv},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\KRYBNGSD\\Bochkovskii 等 - 2024 - Depth Pro Sharp Monocular Metric Depth in Less Than a Second.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\W2RSKNNQ\\2410.html}
}

@misc{brohanRT1RoboticsTransformer2023,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = aug,
  number = {arXiv:2212.06817},
  eprint = {2212.06817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.06817},
  urldate = {2024-12-03},
  abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\N4LZ8M2M\Brohan 等 - 2023 - RT-1 Robotics Transformer for Real-World Control at Scale.pdf}
}

@misc{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15818},
  eprint = {2307.15818},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.15818},
  urldate = {2024-12-03},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\2S75S6LR\Brohan 等 - 2023 - RT-2 Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.pdf}
}

@inproceedings{charlesPointNetDeepLearning2017,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Charles, R. Qi and Su, Hao and Kaichun, Mo and Guibas, Leonidas J.},
  year = {2017},
  month = jul,
  pages = {77--85},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.16},
  urldate = {2025-09-14},
  isbn = {978-1-5386-0457-1},
  file = {C:\Users\guaoxiang\Zotero\storage\GUUUNUQC\Charles 等 - 2017 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf}
}

@misc{cheangGR2GenerativeVideoLanguageAction2024,
  title = {{{GR-2}}: {{A Generative Video-Language-Action Model}} with {{Web-Scale Knowledge}} for {{Robot Manipulation}}},
  shorttitle = {{{GR-2}}},
  author = {Cheang, Chi-Lam and Chen, Guangzeng and Jing, Ya and Kong, Tao and Li, Hang and Li, Yifeng and Liu, Yuxiao and Wu, Hongtao and Xu, Jiafeng and Yang, Yichu and Zhang, Hanbo and Zhu, Minzhao},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06158},
  eprint = {2410.06158},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06158},
  urldate = {2024-12-03},
  abstract = {We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7\% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: https://gr2-manipulation.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8ME7SKHH\Cheang 等 - 2024 - GR-2 A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation.pdf}
}

@article{chernovLeastSquaresFitting2005,
  title = {Least {{Squares Fitting}} of {{Circles}}},
  author = {Chernov, N. and Lesort, C.},
  year = {2005},
  month = nov,
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {23},
  number = {3},
  pages = {239--252},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-005-0482-8},
  urldate = {2025-01-05},
  abstract = {Fitting standard shapes or curves to incomplete data (which represent only a small part of the curve) is a notoriously difficult problem. Even if the curve is quite simple, such as an ellipse or a circle, it is hard to reconstruct it from noisy data sampled along a short arc. Here we study the least squares fit (LSF) of circular arcs to incomplete scattered data. We analyze theoretical aspects of the problem and reveal the cause of unstable behavior of conventional algorithms. We also find a remedy that allows us to build another algorithm that accurately fits circles to data sampled along arbitrarily short arcs.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\DTSR4X6Q\Chernov和Lesort - 2005 - Least Squares Fitting of Circles.pdf}
}

@article{chernovLeastSquaresFitting2005a,
  title = {Least {{Squares Fitting}} of {{Circles}}},
  author = {Chernov, N. and Lesort, C.},
  year = {2005},
  month = nov,
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {23},
  number = {3},
  pages = {239--252},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-005-0482-8},
  urldate = {2025-04-20},
  abstract = {Fitting standard shapes or curves to incomplete data (which represent only a small part of the curve) is a notoriously difficult problem. Even if the curve is quite simple, such as an ellipse or a circle, it is hard to reconstruct it from noisy data sampled along a short arc. Here we study the least squares fit (LSF) of circular arcs to incomplete scattered data. We analyze theoretical aspects of the problem and reveal the cause of unstable behavior of conventional algorithms. We also find a remedy that allows us to build another algorithm that accurately fits circles to data sampled along arbitrarily short arcs.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3I3I3FSA\\Chernov和Lesort - 2005 - Least Squares Fitting of Circles.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\TFPKUJ99\\cl1.zh-CN.mono.pdf}
}

@misc{chiDiffusionPolicyVisuomotor2024,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  year = {2024},
  month = mar,
  number = {arXiv:2303.04137},
  eprint = {2303.04137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  urldate = {2024-12-03},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\HBQLTKTM\2303.04137v5.pdf}
}

@misc{chiDiffusionPolicyVisuomotor2024a,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  year = {2024},
  month = mar,
  number = {arXiv:2303.04137},
  eprint = {2303.04137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  urldate = {2024-12-03},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AMUIUGTW\2303.04137v5.pdf}
}

@misc{chiDiffusionPolicyVisuomotor2024b,
  title = {{Diffusion Policy: Visuomotor Policy Learning via Action Diffusion}},
  shorttitle = {{Diffusion Policy}},
  author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  year = {2024},
  month = mar,
  number = {arXiv:2303.04137},
  eprint = {2303.04137},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04137},
  urldate = {2025-07-15},
  abstract = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9\%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\SL5EVDDW\Chi 等 - 2024 - Diffusion Policy Visuomotor Policy Learning via Action Diffusion.pdf}
}

@article{collinsInfinitesimalPlaneBasedPose2014,
  title = {{Infinitesimal Plane-Based Pose Estimation}},
  author = {Collins, Toby and Bartoli, Adrien},
  year = {2014},
  month = sep,
  journal = {International Journal of Computer Vision},
  volume = {109},
  number = {3},
  pages = {252--286},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-014-0725-5},
  urldate = {2024-11-27},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\K5H34TMH\Collins和Bartoli - 2014 - Infinitesimal Plane-Based Pose Estimation.pdf}
}

@article{coopeCircleFittingLinear1993,
  title = {Circle Fitting by Linear and Nonlinear Least Squares},
  author = {Coope, I. D.},
  year = {1993},
  month = feb,
  journal = {Journal of Optimization Theory and Applications},
  volume = {76},
  number = {2},
  pages = {381--388},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/BF00939613},
  urldate = {2025-01-05},
  abstract = {The problem of determining the circle of best fit to a set of points in the plane (or the obvious generalization to n-dimensions) is easily formulated as a nonlinear total least-squares problem which may be solved using a Gauss-Newton minimization algorithm. This straightforward approach is shown to be inefficient and extremely sensitive to the presence of outliers. An alternative formulation allows the problem to be reduced to a linear least squares problem which is trivially solved. The recommended approach is shown to have the added advantage of being much less sensitive to outliers than the nonlinear least squares approach.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\7CD5BA37\\Coope - 1993 - Circle fitting by linear and nonlinear least squares.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\7V3ZLZRD\\Coope - 1993 - Circle fitting by linear and nonlinear least squares.pdf}
}

@misc{cuiRecentAdvancesSpeech2024,
  title = {Recent {{Advances}} in {{Speech Language Models}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Speech Language Models}}},
  author = {Cui, Wenqian and Yu, Dianzhi and Jiao, Xiaoqi and Meng, Ziqiao and Zhang, Guangyan and Wang, Qichao and Guo, Yiwen and King, Irwin},
  year = {2024},
  month = oct,
  number = {arXiv:2410.03751},
  eprint = {2410.03751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03751},
  urldate = {2024-12-03},
  abstract = {Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)'', where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs)---end-toend models that generate speech without converting from text---have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize the evaluation metrics for SpeechLMs, and discuss the challenges and future research directions in this rapidly evolving field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\guaoxiang\Zotero\storage\69BXDFP8\Cui 等 - 2024 - Recent Advances in Speech Language Models A Survey.pdf}
}

@article{dalalNeuralMPGeneralist,
  title = {Neural {{MP}}: {{A Generalist Neural Motion Planner}}},
  author = {Dalal, Murtaza and Yang, Jiahui and Mendonca, Russell and Khaky, Youssef and Salakhutdinov, Ruslan and Pathak, Deepak},
  abstract = {The current paradigm for motion planning generates solutions from scratch for every new problem, which consumes significant amounts of time and computational resources. For complex, cluttered scenes, motion planning approaches can often take minutes to produce a solution, while humans are able to accurately and safely reach any goal in seconds by leveraging their prior experience. We seek to do the same by applying data-driven learning at scale to the problem of motion planning. Our approach builds a large number of complex scenes in simulation, collects expert data from a motion planner, then distills it into a reactive generalist policy. We then combine this with lightweight optimization to obtain a safe path for real world deployment. We perform a thorough evaluation of our method on 64 motion planning tasks across four diverse environments with randomized poses, scenes and obstacles, in the real world, demonstrating an improvement of 23\%, 17\% and 79\% motion planning success rate over state of the art sampling, optimization and learning based planning methods. Video results available at mihdalal.github.io/neuralmotionplanner.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\CDIS4E3Q\\Dalal 等 - Neural MP A Generalist Neural Motion Planner.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\GE9N5LBM\\Dalal 等 - Neural MP A Generalist Neural Motion Planner.pdf}
}

@article{davisonMonoSLAMRealTimeSingle2007,
  title = {{{MonoSLAM}}: {{Real-Time Single Camera SLAM}}},
  shorttitle = {{{MonoSLAM}}},
  author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {6},
  pages = {1052--1067},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2007.1049},
  urldate = {2024-11-06},
  abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the ``pure vision'' domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\88FCC8FE\Davison 等 - 2007 - MonoSLAM Real-Time Single Camera SLAM.pdf}
}

@inproceedings{delogneComputerOptimizationDeschamps1972,
  title = {Computer Optimization of {{Deschamps}}' Method and Error Cancellation in Reflectometry},
  booktitle = {Proc. {{IMEKO-Symp}}. {{Microwave Measurements}}},
  author = {Delogne, P.},
  year = {1972},
  pages = {117--123}
}

@misc{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03378},
  eprint = {2303.03378},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pretrained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AAEWE6UQ\Driess 等 - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf}
}

@article{du-mingtsaiBoundarybasedCornerDetection1997,
  title = {Boundary-Based Corner Detection Using Neural Networks},
  author = {{Du-Ming Tsai}},
  year = {1997},
  month = jan,
  journal = {Pattern Recognition},
  volume = {30},
  number = {1},
  pages = {85--97},
  issn = {00313203},
  doi = {10.1016/S0031-3203(96)00057-X},
  urldate = {2024-12-07},
  abstract = {In this paper we present a novel boundary-based corner detection approach using artificial neural networks (ANNs). Two neural networks are proposed: one for detecting corner points with high curvature, and the other for detecting tangent points and inflection points that generally have low curvature. For a given boundary point Pi, the first ANN uses the normalized coordinates of points on the forward ann (neighboring points succeeding Pi) or on the backward arm (neighboring points preceding Pi) of the point Pi as the input vector. The output feature of the network is the angle of the forward ann (or backward arm) with respect to the xaxis. The boundary point with sufficiently small angle between the forward and backward arms is identified as a corner. Since the feature points of tangency and inflection have relatively low curvature, the signs of curvature, rather than the magnitude of curvature, for points in the neighborhood of Pi are used as the input vector to the second ANN. The curvature sign at each boundary point is derived from the outputs of the first ANN. The outputs of the second ANN only respond to the sign patterns of tangent points and inflection points. By using both ANNs, all features of corners, tangent points and inflection points can be extracted from the boundary of any arbitrary shape. Experimental results have shown that the proposed ANNs have good detection and localization for objects in random orientations and with moderate scale changes. Copyright {\copyright} 1996. Published by Elsevier Science Ltd.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\P66L53KF\Du-Ming Tsai - 1997 - Boundary-based corner detection using neural networks.pdf}
}

@misc{duLearningUniversalPolicies2023,
  title = {Learning {{Universal Policies}} via {{Text-Guided Video Generation}}},
  author = {Du, Yilun and Yang, Mengjiao and Dai, Bo and Dai, Hanjun and Nachum, Ofir and Tenenbaum, Joshua B. and Schuurmans, Dale and Abbeel, Pieter},
  year = {2023},
  month = nov,
  number = {arXiv:2302.00111},
  eprint = {2302.00111},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00111},
  urldate = {2025-09-16},
  abstract = {A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\CKXFYFUL\\Du 等 - 2023 - Learning Universal Policies via Text-Guided Video Generation.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\JAZQR8RL\\2302.html}
}

@inproceedings{faberBuyersGuideEuclidean2001,
  title = {A {{Buyer}}'s {{Guide}} to {{Euclidean Elliptical Cylindrical}} and {{Conical Surface Fitting}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2001},
  author = {Faber, P and Fisher, R B},
  year = {2001},
  pages = {54.1-54.10},
  publisher = {British Machine Vision Association},
  address = {Manchester},
  doi = {10.5244/C.15.54},
  urldate = {2025-01-06},
  abstract = {The ability to construct CAD or other object models from edge and range data has a fundamental meaning in building a recognition and positioning system. While the problem of model fitting has been successfully addressed, the problem of efficient high accuracy and stability of the fitting is still an open problem. In the past researchers have used approximate distance functions rather than the real Euclidean distance because of computational efficiency. We now feel that machine speeds are sufficient to ask whether it is worth considering Euclidean fitting again. This paper address the problem of estimation of elliptical cylinder and cone surfaces to 3D data by a constrained Euclidean fitting. We study and compare the performance of various distance functions in terms of correctness, robustness and pose invariance, and present our results improving known fitting methods by closed form expressions of the real Euclidean distance.},
  isbn = {978-1-901725-16-2},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\UMBX5M7V\Faber和Fisher - 2001 - A Buyer's Guide to Euclidean Elliptical Cylindrical and Conical Surface Fitting.pdf}
}

@article{feiNewShortarcFitting2020,
  title = {A New Short-Arc Fitting Method with High Precision Using {{Adam}} Optimization Algorithm},
  author = {Fei, Zhigen and Wu, Zhiying and Xiao, Yanqiu and Ma, Jun and He, Wenbin},
  year = {2020},
  month = jun,
  journal = {Optik},
  volume = {212},
  pages = {164788},
  issn = {0030-4026},
  doi = {10.1016/j.ijleo.2020.164788},
  urldate = {2024-11-20},
  abstract = {The high-precision short-arc measurement is a huge challenge in scientific research and engineering practice. The popular traditional least square fitting (TLSF) however fails to achieve the precise fitting parameters when the arc gets shorter. In this work, an inequation constrained fitting (ICF) method based on four-parameter circle equation is presented. Lagrangian multiplier and Karush-Kuhn-Tucker criteria are used to redefine the objective function. After that, Adam algorithm is utilized to solve the objective function in iterative way. Adam algorithm has a strong ability to resist noise pollution by virtue of modifying continually the first-order momentum and second-order momentum with average of gradients during the course of iteration. Finally, simulation and experimental results show that our ICF method is more robust and high-precision than TLSF and Hyper method, so it is very competent to measure short arcs with noise even their central angles are close to 5{$^\circ$}.},
  langid = {american},
  keywords = {,Adam algorithm,Lagrangian multiplier,Short-arc fitting},
  file = {C:\Users\guaoxiang\Zotero\storage\36ADMCMA\S0030402620306240.html}
}

@article{feiNewShortarcFitting2020a,
  title = {{A new short-arc fitting method with high precision using Adam optimization algorithm}},
  author = {Fei, Zhigen and Wu, Zhiying and Xiao, Yanqiu and Ma, Jun and He, Wenbin},
  year = {2020},
  month = jun,
  journal = {Optik},
  volume = {212},
  pages = {164788},
  issn = {00304026},
  doi = {10.1016/j.ijleo.2020.164788},
  urldate = {2025-04-20},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\Z3QHDW2N\Fei 等 - 2020 - A new short-arc fitting method with high precision using Adam optimization algorithm.pdf}
}

@article{feiNewShortarcFitting2020b,
  title = {A New Short-Arc Fitting Method with High Precision Using {{Adam}} Optimization Algorithm},
  author = {Fei, Zhigen and Wu, Zhiying and Xiao, Yanqiu and Ma, Jun and He, Wenbin},
  year = {2020},
  month = jun,
  journal = {Optik},
  volume = {212},
  pages = {164788},
  issn = {00304026},
  doi = {10.1016/j.ijleo.2020.164788},
  urldate = {2024-11-20},
  abstract = {The high-precision short-arc measurement is a huge challenge in scientific research and engineering practice. The popular traditional least square fitting (TLSF) however fails to achieve the precise fitting parameters when the arc gets shorter. In this work, an inequation constrained fitting (ICF) method based on four-parameter circle equation is presented. Lagrangian multiplier and Karush-Kuhn-Tucker criteria are used to redefine the objective function. After that, Adam algorithm is utilized to solve the objective function in iterative way. Adam algorithm has a strong ability to resist noise pollution by virtue of modifying continually the first-order momentum and second-order momentum with average of gradients during the course of iteration. Finally, simulation and experimental results show that our ICF method is more robust and high-precision than TLSF and Hyper method, so it is very competent to measure short arcs with noise even their central angles are close to 5{$^\circ$}.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\D3XCIPIC\\use.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\ZFA4JE24\\Fei 等 - 2020 - A new short-arc fitting method with high precision using Adam optimization algorithm.pdf}
}

@misc{firooziFoundationModelsRobotics2023,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper (Preliminary release. We are committed to further enhancing and updating this work to ensure its quality and relevance) can be found here: https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6EADR4YW\\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\C7KSUGVZ\\2312.html}
}

@misc{firooziFoundationModelsRobotics2023a,
  title = {Foundation {{Models}} in {{Robotics}}: {{Applications}}, {{Challenges}}, and the {{Future}}},
  shorttitle = {Foundation {{Models}} in {{Robotics}}},
  author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and Ichter, Brian and Driess, Danny and Wu, Jiajun and Lu, Cewu and Schwager, Mac},
  year = {2023},
  month = dec,
  number = {arXiv:2312.07843},
  eprint = {2312.07843},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {We survey applications of pretrained foundation models in robotics. Traditional deep learning models in robotics are trained on small datasets tailored for specific tasks, which limits their adaptability across diverse applications. In contrast, foundation models pretrained on internet-scale data appear to have superior generalization capabilities, and in some instances display an emergent ability to find zero-shot solutions to problems that are not present in the training data. Foundation models may hold the potential to enhance various components of the robot autonomy stack, from perception to decision-making and control. For example, large language models can generate code or provide common sense reasoning, while vision-language models enable open-vocabulary visual recognition. However, significant open research challenges remain, particularly around the scarcity of robot-relevant training data, safety guarantees and uncertainty quantification, and real-time execution. In this survey, we study recent papers that have used or built foundation models to solve robotics problems. We explore how foundation models contribute to improving robot capabilities in the domains of perception, decision-making, and control. We discuss the challenges hindering the adoption of foundation models in robot autonomy and provide opportunities and potential pathways for future advancements. The GitHub project corresponding to this paper1 can be found here.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\97V5HWLE\Firoozi 等 - 2023 - Foundation Models in Robotics Applications, Challenges, and the Future.pdf}
}

@article{fitzgibbonDirectLeastSquare1999,
  title = {{Direct least square fitting of ellipses}},
  author = {Fitzgibbon, A. and Pilu, M. and Fisher, R.B.},
  year = {1999},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {5},
  pages = {476--480},
  issn = {01628828},
  doi = {10.1109/34.765658},
  urldate = {2024-11-27},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\LUJ4TW3Z\Fitzgibbon 等 - 1999 - Direct least square fitting of ellipses.pdf}
}

@misc{fuMobileALOHALearning2024,
  title = {Mobile {{ALOHA}}: {{Learning Bimanual Mobile Manipulation}} with {{Low-Cost Whole-Body Teleoperation}}},
  shorttitle = {Mobile {{ALOHA}}},
  author = {Fu, Zipeng and Zhao, Tony Z. and Finn, Chelsea},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02117},
  eprint = {2401.02117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.02117},
  urldate = {2024-12-03},
  abstract = {Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system [104] with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90\%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\guaoxiang\Zotero\storage\MX6MQSNW\Fu 等 - 2024 - Mobile ALOHA Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation.pdf}
}

@misc{garciaGeneralizableVisionLanguageRobotic2024,
  title = {Towards {{Generalizable Vision-Language Robotic Manipulation}}: {{A Benchmark}} and {{LLM-guided 3D Policy}}},
  shorttitle = {Towards {{Generalizable Vision-Language Robotic Manipulation}}},
  author = {Garcia, Ricardo and Chen, Shizhe and Schmid, Cordelia},
  year = {2024},
  month = oct,
  number = {arXiv:2410.01345},
  eprint = {2410.01345},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Generalizing language-conditioned robotic policies to new tasks remains a significant challenge, hampered by the lack of suitable simulation benchmarks. In this paper, we address this gap by introducing GemBench, a novel benchmark to assess generalization capabilities of vision-language robotic manipulation policies. GemBench incorporates seven general action primitives and four levels of generalization, spanning novel placements, rigid and articulated objects, and complex long-horizon tasks. We evaluate state-of-the-art approaches on GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich 3D information for action prediction conditioned on language. While 3D-LOTUS excels in both efficiency and performance on seen tasks, it struggles with novel tasks. To address this, we present 3D-LOTUS++, a framework that integrates 3D-LOTUS's motion planning capabilities with the task planning capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++ achieves state-ofthe-art performance on novel tasks of GemBench, setting a new standard for generalization in robotic manipulation. The benchmark, codes and trained models are available at https: //www.di.ens.fr/willow/research/gembench/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\USPYPEPA\Garcia 等 - 2024 - Towards Generalizable Vision-Language Robotic Manipulation A Benchmark and LLM-guided 3D Policy.pdf}
}

@misc{ghojoghEigenvalueGeneralizedEigenvalue2023,
  title = {{Eigenvalue and Generalized Eigenvalue Problems: Tutorial}},
  shorttitle = {{Eigenvalue and Generalized Eigenvalue Problems}},
  author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
  year = {2023},
  month = may,
  number = {arXiv:1903.11240},
  eprint = {1903.11240},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.11240},
  urldate = {2025-04-22},
  abstract = {This paper is a tutorial for eigenvalue and generalized eigenvalue problems. We first introduce eigenvalue problem, eigen-decomposition (spectral decomposition), and generalized eigenvalue problem. Then, we mention the optimization problems which yield to the eigenvalue and generalized eigenvalue problems. We also provide examples from machine learning, including principal component analysis, kernel supervised principal component analysis, and Fisher discriminant analysis, which result in eigenvalue and generalized eigenvalue problems. Finally, we introduce the solutions to both eigenvalue and generalized eigenvalue problems.},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\IRCEB7ZY\Ghojogh 等 - 2023 - Eigenvalue and Generalized Eigenvalue Problems Tutorial.pdf}
}

@article{gorokhovatskyiSearchVisualObjects2023,
  title = {Search for {{Visual Objects}} by {{Request}} in the {{Form}} of a {{Cluster Representation}} for the {{Structural Image Description}}},
  author = {Gorokhovatskyi, Volodymyr and Tvoroshenko, Iryna and Kobylin, Oleg and Vlasenko, Nataliia},
  year = {2023},
  month = may,
  journal = {Advances in Electrical and Electronic Engineering},
  volume = {21},
  number = {1},
  pages = {19--27},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v21i1.4661},
  urldate = {2024-11-07},
  abstract = {The key task of computer vision is the recognition of visual objects in the analysed image. This paper proposes a method of searching for objects in an image, based on the identification of a cluster representation of the query descriptions and the current image of the window with the calculation of the relevance measure. The implementation of a cluster representation significantly increases the speed of identification or classification of visual objects while maintaining a sufficient level of accuracy. Based on the development of models for the analysis and processing of a set of descriptors of keypoints, we have obtained an effective method for the identification of visual objects. A comparative experiment with the traditional method has been conducted, where a linear search for the nearest descriptor was implemented for identification without using a cluster representation of the description. In the experiment, a speed gain for the developed method has been obtained in comparison with the traditional one by approximately 5.2 times with the same level of accuracy. The method can be used in applied tasks where the time of object identification is critical. The developed method can be applied to search for several objects of different classes. The effectiveness of the method can be increased by varying the values of its parameters and adapting to the characteristics of the data.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\NJAIKU54\Gorokhovatskyi 等 - 2023 - Search for Visual Objects by Request in the Form of a Cluster Representation for the Structural Imag.pdf}
}

@article{halirVNUMERICALLYSTABLEDIRECT,
  title = {{{NUMERICALLY STABLE DIRECT LEAST SQUARES FITTING OF ELLIPSES}}},
  author = {Hal{\i}r{\textasciicaron}, Radim and Flusser, Jan},
  abstract = {This paper presents a numerically stable non-iterative algorithm for fitting an ellipse to a set of data points. The approach is based on a least squares minimization and it guarantees an ellipse-specific solution even for scattered or noisy data. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method which can be easily implemented. The proposed algorithm has no computational ambiguity and it is able to fit more than 100,000 points in a second.},
  langid = {english}
}

@misc{hannunDeepSpeechScaling2014,
  title = {Deep {{Speech}}: {{Scaling}} up End-to-End Speech Recognition},
  shorttitle = {Deep {{Speech}}},
  author = {Hannun, Awni and Case, Carl and Casper, Jared and Catanzaro, Bryan and Diamos, Greg and Elsen, Erich and Prenger, Ryan and Satheesh, Sanjeev and Sengupta, Shubho and Coates, Adam and Ng, Andrew Y.},
  year = {2014},
  month = dec,
  number = {arXiv:1412.5567},
  eprint = {1412.5567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.5567},
  urldate = {2024-12-03},
  abstract = {We present a state-of-the-art speech recognition system developed using end-toend deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a ``phoneme.'' Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\guaoxiang\Zotero\storage\AQS7RZGY\Hannun 等 - 2014 - Deep Speech Scaling up end-to-end speech recognition.pdf}
}

@misc{huangInstruct2ActMappingMultimodality2023,
  title = {{{Instruct2Act}}: {{Mapping Multi-modality Instructions}} to {{Robotic Actions}} with {{Large Language Model}}},
  shorttitle = {{{Instruct2Act}}},
  author = {Huang, Siyuan and Jiang, Zhengkai and Dong, Hao and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  year = {2023},
  month = may,
  number = {arXiv:2305.11176},
  eprint = {2305.11176},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.11176},
  urldate = {2024-12-03},
  abstract = {Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\GZ4YRD76\Huang 等 - 2023 - Instruct2Act Mapping Multi-modality Instructions to Robotic Actions with Large Language Model.pdf}
}

@misc{huangManipVQAInjectingRobotic2024,
  title = {{{ManipVQA}}: {{Injecting Robotic Affordance}} and {{Physically Grounded Information}} into {{Multi-Modal Large Language Models}}},
  shorttitle = {{{ManipVQA}}},
  author = {Huang, Siyuan and Ponomarenko, Iaroslav and Jiang, Zhengkai and Li, Xiaoqi and Hu, Xiaobin and Gao, Peng and Li, Hongsheng and Dong, Hao},
  year = {2024},
  month = aug,
  number = {arXiv:2403.11289},
  eprint = {2403.11289},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.11289},
  urldate = {2024-12-03},
  abstract = {While the integration of Multi-modal Large Language Models (MLLMs) with robotic systems has significantly improved robots' ability to understand and execute natural language instructions, their performance in manipulation tasks remains limited due to a lack of robotics-specific knowledge. Conventional MLLMs are typically trained on generic imagetext pairs, leaving them deficient in understanding affordances and physical concepts crucial for manipulation. To address this gap, we propose ManipVQA, a novel framework that infuses MLLMs with manipulation-centric knowledge through a Visual Question-Answering (VQA) format. This approach encompasses tool detection, affordance recognition, and a broader understanding of physical concepts. We curated a diverse dataset of images depicting interactive objects, to challenge robotic understanding in tool detection, affordance prediction, and physical concept comprehension. To effectively integrate this robotics-specific knowledge with the inherent vision-reasoning capabilities of MLLMs, we leverage a unified VQA format and devise a fine-tuning strategy. This strategy preserves the original vision-reasoning abilities while incorporating the newly acquired robotic insights. Empirical evaluations conducted in robotic simulators and across various vision task benchmarks demonstrate the robust performance of ManipVQA. The code and dataset are publicly available at https://github.com/SiyuanHuang95/ManipVQA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@article{huangNovelAlgorithmFitting2021,
  title = {A Novel Algorithm: Fitting a Spatial Arc to Noisy Point Clouds with High Accuracy and Reproducibility},
  shorttitle = {A Novel Algorithm},
  author = {Huang, Shuai and Chen, Ming and Lu, ShengLian and Chen, ShouXin and Zha, YongJian},
  year = {2021},
  month = aug,
  journal = {Measurement Science and Technology},
  volume = {32},
  number = {8},
  pages = {085004},
  issn = {0957-0233, 1361-6501},
  doi = {10.1088/1361-6501/abf867},
  urldate = {2024-11-20},
  abstract = {Fitting a spatial arc to noisy point clouds with high accuracy and reproducibility is challenging, although it is important in many applications, such as precise measurement, computerized numerical control machining and robotic path planning. In optical measuring applications, an arc-shaped object is usually first scanned as point clouds by a 3D camera or multiple charge-coupled device cameras, and arc fitting is then invoked to fit these point clouds, obtaining the measuring radius and center. The accuracy of the arc-fitting algorithm plays an important role in the arc-measuring precision. In this paper, a novel algorithm is proposed to fit a spatial arc of high accuracy and reproducibility to noisy point clouds. This method combines the repeated least trimmed squares idea with the smoothing fairness function, i.e. discrete curvature, to devise the objective function, which is solved iteratively. This algorithm can successfully filter noise and fit a highly accurate arc to noisy point clouds with high reproducibility. Seven popular arc-fitting algorithms are implemented as benchmarks and both simulated and real data scanned from physical objects are tested to validate that the proposed algorithm performs best. The proposed algorithm is efficient and can be easily implemented in industrial applications.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6SMGANIU\\Huang 等 - 2021 - A novel algorithm fitting a spatial arc to noisy point clouds with high accuracy and reproducibilit.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\H2GGVS86\\Huang 等 - 2021 - A novel algorithm fitting a spatial arc to noisy point clouds with high accuracy and reproducibilit.pdf}
}

@misc{huangReKepSpatioTemporalReasoning2024,
  title = {{{ReKep}}: {{Spatio-Temporal Reasoning}} of {{Relational Keypoint Constraints}} for {{Robotic Manipulation}}},
  shorttitle = {{{ReKep}}},
  author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and {Fei-Fei}, Li},
  year = {2024},
  month = nov,
  number = {arXiv:2409.01652},
  eprint = {2409.01652},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.01652},
  urldate = {2024-12-03},
  abstract = {Representing robotic manipulation tasks as constraints that associate the robot and the environment is a promising way to encode desired robot behaviors. However, it remains unclear how to formulate the constraints such that they are 1) versatile to diverse tasks, 2) free of manual labeling, and 3) optimizable by off-the-shelf solvers to produce robot actions in real-time. In this work, we introduce Relational Keypoint Constraints (ReKep), a visually-grounded representation for constraints in robotic manipulation. Specifically, ReKep is expressed as Python functions mapping a set of 3D keypoints in the environment to a numerical cost. We demonstrate that by representing a manipulation task as a sequence of Relational Keypoint Constraints, we can employ a hierarchical optimization procedure to solve for robot actions (represented by a sequence of end-effector poses in SE(3)) with a perception-action loop at a real-time frequency. Furthermore, in order to circumvent the need for manual specification of ReKep for each new task, we devise an automated procedure that leverages large vision models and vision-language models to produce ReKep from free-form language instructions and RGB-D observations. We present system implementations on a wheeled single-arm platform and a stationary dual-arm platform that can perform a large variety of manipulation tasks, featuring multi-stage, in-the-wild, bimanual, and reactive behaviors, all without task-specific data or environment models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\4WG6ERKS\Huang 等 - 2024 - ReKep Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation.pdf}
}

@misc{huangVoxPoserComposable3D2023,
  title = {{{VoxPoser}}: {{Composable 3D Value Maps}} for {{Robotic Manipulation}} with {{Language Models}}},
  shorttitle = {{{VoxPoser}}},
  author = {Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and {Fei-Fei}, Li},
  year = {2023},
  month = nov,
  number = {arXiv:2307.05973},
  eprint = {2307.05973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a largescale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at voxposer.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\CANL9GNR\Huang 等 - 2023 - VoxPoser Composable 3D Value Maps for Robotic Manipulation with Language Models.pdf}
}

@misc{huaREDEEndendObject2021,
  title = {{{REDE}}: {{End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}}},
  shorttitle = {{{REDE}}},
  author = {Hua, Weitong and Zhou, Zhongxiang and Wu, Jun and Huang, Huang and Wang, Yue and Xiong, Rong},
  year = {2021},
  month = feb,
  eprint = {2010.12807},
  primaryclass = {cs},
  doi = {10.1109/LRA.2021.3062304},
  urldate = {2024-11-01},
  abstract = {Object 6D pose estimation is a fundamental task in many applications. Conventional methods solve the task by detecting and matching the keypoints, then estimating the pose. Recent efforts bringing deep learning into the problem mainly overcome the vulnerability of conventional methods to environmental variation due to the hand-crafted feature design. However, these methods cannot achieve end-to-end learning and good interpretability at the same time. In this paper, we propose REDE, a novel end-to-end object pose estimator using RGB-D data, which utilizes network for keypoint regression, and a differentiable geometric pose estimator for pose error back-propagation. Besides, to achieve better robustness when outlier keypoint prediction occurs, we further propose a differentiable outliers elimination method that regresses the candidate result and the confidence simultaneously. Via confidence weighted aggregation of multiple candidates, we can reduce the effect from the outliers in the final estimation. Finally, following the conventional method, we apply a learnable refinement process to further improve the estimation. The experimental results on three benchmark datasets show that REDE slightly outperforms the state-of-the-art approaches and is more robust to object occlusion. Our code is available at https://github.com/HuaWeitong/REDE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\CQTLNTZQ\Hua 等 - 2021 - REDE End-to-end Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination.pdf}
}

@article{hubertDeterministicAlgorithmRobust,
  title = {A Deterministic Algorithm for Robust Location and Scatter},
  author = {Hubert, Mia},
  abstract = {Most algorithms for highly robust estimators of multivariate location and scatter start by drawing a large number of random subsets. For instance, the FASTMCD algorithm of Rousseeuw and Van Driessen (1999) starts in this way, and then takes so-called concentration steps to obtain a more accurate approximation to the MCD. The FASTMCD algorithm is affine equivariant but not permutation invariant. In this article we present a deterministic algorithm, denoted as DetMCD, which does not use random subsets and is even faster. It computes a small number of deterministic initial estimators, followed by concentration steps. DetMCD is permutation invariant and very close to affine equivariant. We compare it to FASTMCD and to the OGK estimator of Maronna and Zamar (2002). We also illustrate it on real and simulated data sets, with applications involving principal component analysis, classification and time series analysis. Supplemental material (Matlab code of the DetMCD algorithm and the data sets) is available online.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\ADKAWM6P\\Hubert - A deterministic algorithm for robust location and scatter.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\ZKYI7RXH\\translated_A-Deterministic-Algorithm-for-Robust-Location-and-Scatter.pdf}
}

@article{hubertMinimumCovarianceDeterminant2018,
  title = {Minimum {{Covariance Determinant}} and {{Extensions}}},
  author = {Hubert, Mia and Debruyne, Michiel and Rousseeuw, Peter J.},
  year = {2018},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {10},
  number = {3},
  eprint = {1709.07045},
  primaryclass = {stat},
  pages = {e1421},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1421},
  urldate = {2024-12-27},
  abstract = {The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. Since estimating the covariance matrix is the cornerstone of many multivariate statistical methods, the MCD is an important building block when developing robust multivariate techniques. It also serves as a convenient and efficient tool for outlier detection. The MCD estimator is reviewed, along with its main properties such as affine equivariance, breakdown value, and influence function. We discuss its computation, and list applications and extensions of the MCD in applied and methodological multivariate statistics. Two recent extensions of the MCD are described. The first one is a fast deterministic algorithm which inherits the robustness of the MCD while being almost affine equivariant. The second is tailored to high-dimensional data, possibly with more dimensions than cases, and incorporates regularization to prevent singular matrices.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\guaoxiang\Zotero\storage\LV4ZA43Y\Hubert 等 - 2018 - Minimum Covariance Determinant and Extensions.pdf}
}

@article{hubertROBPCANewApproach2005,
  title = {{{ROBPCA}}: {{A New Approach}} to {{Robust Principal Component Analysis}}},
  shorttitle = {{{ROBPCA}}},
  author = {Hubert, Mia and Rousseeuw, Peter J and Vanden Branden, Karlien},
  year = {2005},
  month = feb,
  journal = {Technometrics},
  volume = {47},
  number = {1},
  pages = {64--79},
  issn = {0040-1706, 1537-2723},
  doi = {10.1198/004017004000000563},
  urldate = {2024-12-27},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\NMHUFRWH\Hubert 等 - 2005 - ROBPCA A New Approach to Robust Principal Component Analysis.pdf}
}

@article{hussainYOLOv1YOLOv8Rise2023,
  title = {{YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and Industrial Defect Detection}},
  author = {Hussain, Muhammad},
  year = {2023},
  month = jun,
  journal = {Machines},
  volume = {11},
  number = {7},
  pages = {677},
  issn = {2075-1702},
  doi = {10.3390/machines11070677},
  urldate = {2025-05-23},
  abstract = {Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has rapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned by the principle of real-time and high-classification performance, based on limited but efficient computational parameters. This principle has been found within the DNA of all YOLO variants with increasing intensity, as the variants evolve addressing the requirements of automated quality inspection within the industrial surface defect detection domain, such as the need for fast detection, high accuracy, and deployment onto constrained edge devices. This paper is the first to provide an in-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from the perspective of industrial manufacturing. The review explores the key architectural advancements proposed at each iteration, followed by examples of industrial deployment for surface defect detection endorsing its compatibility with industrial requirements.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\KLKN5IBE\Hussain - 2023 - YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Complementary Nature toward Digital Manufacturing and I.pdf}
}

@article{jauhriRobotLearningMobile2022,
  title = {Robot {{Learning}} of {{Mobile Manipulation}} with {{Reachability Behavior Priors}}},
  author = {Jauhri, Snehal and Peters, Jan and Chalvatzaki, Georgia},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  eprint = {2203.04051},
  primaryclass = {cs},
  pages = {8399--8406},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3188109},
  urldate = {2024-09-25},
  abstract = {Mobile Manipulation (MM) systems are ideal candidates for taking up the role of personal assistants in unstructured real-world environments. MM requires effective coordination of the robot's embodiments for tasks that require both mobility and manipulation. Reinforcement Learning (RL) holds the promise of endowing robots with adaptive behaviors, but most methods require large amounts of data. In this work, we study the integration of robotic reachability priors in actor-critic RL methods for accelerating the learning of MM for reaching and fetching tasks. Namely, we consider the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target. We devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization. Next, we train a reachability prior using data from the operational robot workspace, inspired by classical methods. Subsequently, we derive Boosted HyRL (BHyRL), a novel actor-critic algorithm that benefits from modeling Q-functions as a sum of residuals. For every new task, we transfer our learned residuals and learn the component of the Q-function that is task-specific, hence, maintaining the task structure from prior behaviors. Moreover, we find that regularizing the target policy with a prior policy yields more expressive behaviors. We evaluate our method in simulation in reaching and fetching tasks of increasing difficulty, and show the superior performance of BHyRL against baseline methods. Finally, we zero-transfer our learned 6D fetching policy with BHyRL to our MM robot: TIAGo++. For more details, refer to our project site: https://irosalab.com/rlmmbp.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {TLDR: This work considers the problem of optimal base placement and the subsequent decision of whether to activate the arm for reaching a 6D target, and devise a novel Hybrid RL (HyRL) method that handles discrete and continuous actions jointly, resorting to the Gumbel-Softmax reparameterization.},
  file = {C:\Users\guaoxiang\Zotero\storage\3FUWWX5B\Jauhri 等 - 2022 - Robot Learning of Mobile Manipulation with Reachability Behavior Priors.pdf}
}

@article{jiangReviewYoloAlgorithm2022a,
  title = {{A Review of Yolo Algorithm Developments}},
  author = {Jiang, Peiyuan and Ergu, Daji and Liu, Fangyao and Cai, Ying and Ma, Bo},
  year = {2022},
  journal = {Procedia Computer Science},
  volume = {199},
  pages = {1066--1073},
  issn = {18770509},
  doi = {10.1016/j.procs.2022.01.135},
  urldate = {2025-05-23},
  langid = {chinese}
}

@misc{jiangRTMPoseRealTimeMultiPerson2023,
  title = {{{RTMPose}}: {{Real-Time Multi-Person Pose Estimation}} Based on {{MMPose}}},
  shorttitle = {{{RTMPose}}},
  author = {Jiang, Tao and Lu, Peng and Zhang, Li and Ma, Ningsheng and Han, Rui and Lyu, Chengqi and Li, Yining and Chen, Kai},
  year = {2023},
  month = jul,
  number = {arXiv:2303.07399},
  eprint = {2303.07399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Recent studies on 2D pose estimation have achieved excellent performance on public benchmarks, yet its application in the industrial community still suffers from heavy model parameters and high latency. To bridge this gap, we empirically explore key factors in pose estimation including paradigm, model architecture, training strategy, and deployment, and present a high-performance real-time multiperson pose estimation framework, RTMPose, based on MMPose. Our RTMPose-m achieves 75.8\% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU, and RTMPose-x achieves 65.3\% AP on COCO-WholeBody. To further evaluate RTMPose's capability in critical real-time applications, we also report the performance after deploying on the mobile device. Our RTMPose-s model achieves 72.2\% AP on COCO with 70+ FPS on a Snapdragon 865 chip, outperforming existing open-source libraries. Our code and models are available at https://github.com/openmmlab/mmpose/tree/main/projects/rtmpose.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\LPV75RBD\Jiang 等 - 2023 - RTMPose Real-Time Multi-Person Pose Estimation based on MMPose.pdf}
}

@misc{jiangVIMAGeneralRobot2023,
  title = {{{VIMA}}: {{General Robot Manipulation}} with {{Multimodal Prompts}}},
  shorttitle = {{{VIMA}}},
  author = {Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and {Fei-Fei}, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  year = {2023},
  month = may,
  number = {arXiv:2210.03094},
  eprint = {2210.03094},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. This work shows that we can express a wide spectrum of robot manipulation tasks with multimodal prompts, interleaving textual and visual tokens. We design a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. To train and evaluate VIMA, we develop a new simulation benchmark with thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and four levels of evaluation protocol for systematic generalization. VIMA achieves strong scalability in both model capacity and data size. It outperforms prior SOTA methods in the hardest zero-shot generalization setting by up to 2.9{\texttimes} task success rate given the same training data. With 10{\texttimes} less training data, VIMA still performs 2.7{\texttimes} better than the top competing approach. We open-source all code, pretrained models, dataset, and simulation benchmark at https://vimalabs.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\293Q63E9\Jiang 等 - 2023 - VIMA General Robot Manipulation with Multimodal Prompts.pdf}
}

@misc{JiGuangSanWeiDianYunXiaJiJieLingJianBiaoMianQueXianShiBieFangFaZhongGuoZhiWang,
  title = {激光三维点云下机械零件表面缺陷识别方法 - 中国知网},
  urldate = {2025-03-25},
  howpublished = {https://kns.cnki.net/kcms2/article/abstract?v=\_GofKS1StuRHdAUcmYizkTywnOSyddY7dxQVFu3rMbyR8oarC1yujQe\_ORURcxTKGKsRLEog1KwSfiYFgmBscGRMXR\_OgLZO1Z3IDrYfB7hT0aZJo4kDqfJcdAdMWjVFSfoo5ErQ4PJKYNO4sY\_EZftXBtBqrYYV8FdmgF\_nWUIAO-Go5BBAswmAQdCgnEFt\&uniplatform=NZKPT\&language=CHS},
  file = {C:\Users\guaoxiang\Zotero\storage\YJ4LG5SY\abstract.html}
}

@inproceedings{kalashnikovScalableDeepReinforcement2018,
  title = {Scalable {{Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Robot Learning}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  year = {2018},
  month = oct,
  pages = {651--673},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-10-13},
  abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\C5D82LVY\Kalashnikov 等 - 2018 - Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation.pdf}
}

@article{kasaCircleFittingProcedure1976,
  title = {A Circle Fitting Procedure and Its Error Analysis},
  author = {K{\aa}sa, I.},
  year = {1976},
  journal = {IEEE Transactions on instrumentation and measurement},
  number = {1},
  pages = {8--14},
  publisher = {IEEE},
  isbn = {0018-9456}
}

@article{kilambiDevelopmentAlgorithmMeasure2012,
  title = {Development of an Algorithm to Measure Defect Geometry Using a {{3D}} Laser Scanner},
  author = {Kilambi, S and Tipton, S M},
  year = {2012},
  month = aug,
  journal = {Measurement Science and Technology},
  volume = {23},
  number = {8},
  pages = {085604},
  issn = {0957-0233, 1361-6501},
  doi = {10.1088/0957-0233/23/8/085604},
  urldate = {2025-04-20},
  abstract = {Current fatigue life prediction models for coiled tubing (CT) require accurate measurements of the defect geometry. Three-dimensional (3D) laser imaging has shown promise toward becoming a nondestructive, non-contacting method of surface defect characterization. Laser imaging provides a detailed photographic image of a flaw, in addition to a detailed 3D surface map from which its critical dimensions can be measured. This paper describes algorithms to determine defect characteristics, specifically depth, width, length and projected cross-sectional area. Curve-fitting methods were compared and implicit algebraic fits have higher probability of convergence compared to explicit geometric fits. Among the algebraic fits, the Taubin circle fit has the least error. The algorithm was able to extract the dimensions of the flaw geometry from the scanned data of CT to within a tolerance of about 0.127 mm, close to the tolerance specified for the laser scanner itself, compared to measurements made using traveling microscopes. The algorithm computes the projected surface area of the flaw, which could previously only be estimated from the dimension measurements and the assumptions made about cutter shape. Although shadows compromised the accuracy of the shape characterization, especially for deep and narrow flaws, the results indicate that the algorithm with laser scanner can be used for non-destructive evaluation of CT in the oil field industry. Further work is needed to improve accuracy, to eliminate shadow effects and to reduce radial deviation.},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\2ALUND3Y\\Kilambi和Tipton - 2012 - Development of an algorithm to measure defect geometry using a 3D laser scanner.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\C6WDTEBE\\Kilambi_2012_Meas._Sci._Technol._23_085604.zh-CN.mono.pdf}
}

@misc{kimOpenVLAOpenSourceVisionLanguageAction2024,
  title = {{{OpenVLA}}: {{An Open-Source Vision-Language-Action Model}}},
  shorttitle = {{{OpenVLA}}},
  author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  year = {2024},
  month = sep,
  number = {arXiv:2406.09246},
  eprint = {2406.09246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.09246},
  urldate = {2024-12-03},
  abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8ENWJAT2\Kim 等 - 2024 - OpenVLA An Open-Source Vision-Language-Action Model.pdf}
}

@misc{leyderRobPyPythonPackage2024,
  title = {{{RobPy}}: A {{Python Package}} for {{Robust Statistical Methods}}},
  shorttitle = {{{RobPy}}},
  author = {Leyder, Sarah and Raymaekers, Jakob and Rousseeuw, Peter J. and Servotte, Thomas and Verdonck, Tim},
  year = {2024},
  month = nov,
  number = {arXiv:2411.01954},
  eprint = {2411.01954},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.01954},
  urldate = {2025-01-06},
  abstract = {Robust estimation provides essential tools for analyzing data that contain outliers, ensuring that statistical models remain reliable even in the presence of some anomalous data. While robust methods have long been available in R, Python users have lacked a comprehensive package that offers these methods in a cohesive framework. RobPy addresses this gap by offering a wide range of robust methods in Python, built upon established libraries including NumPy, SciPy, and scikit-learn. This package includes tools for robust preprocessing, univariate estimation, covariance matrices, regression, and principal component analysis, which are able to detect outliers and to mitigate their effect. In addition, RobPy provides specialized diagnostic plots for visualizing outliers in both casewise and cellwise contexts. This paper presents the structure of the RobPy package, demonstrates its functionality through examples, and compares its features to existing implementations in other statistical software. By bringing robust methods to Python, RobPy enables more users to perform robust data analysis in a modern and versatile programming language.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\E7TXWDAI\Leyder 等 - 2024 - RobPy a Python Package for Robust Statistical Methods.pdf}
}

@misc{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2023},
  month = may,
  number = {arXiv:2209.07753},
  eprint = {2209.07753},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\EMNPHLHU\Liang 等 - 2023 - Code as Policies Language Model Programs for Embodied Control.pdf}
}

@misc{liManipLLMEmbodiedMultimodal2023,
  title = {{{ManipLLM}}: {{Embodied Multimodal Large Language Model}} for {{Object-Centric Robotic Manipulation}}},
  shorttitle = {{{ManipLLM}}},
  author = {Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.16217},
  eprint = {2312.16217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming waypoints in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of ManipLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\D2Y4PPCK\Li 等 - 2023 - ManipLLM Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation.pdf}
}

@article{limbergerRealTimeDetectionPlanar,
  title = {Real-{{Time Detection}} of {{Planar Regions}} in {{Unorganized Point Clouds}}},
  author = {Limberger, Frederico Artur},
  abstract = {Automatic detection of planar regions in point clouds is an important step for many graphics, image processing, and computer vision applications. While laser scanners and digital photography have allowed us to capture increasingly larger datasets, previous techniques are computationally expensive, being unable to achieve real-time performance for datasets containing tens of thousands of points, even when detection is performed in a non-deterministic way. We present a deterministic technique for plane detection in unorganized point clouds whose cost is O(n log n) in the number of input samples. It is based on an efficient Hough-transform voting scheme and works by clustering approximately co-planar points and by casting votes for these clusters on a spherical accumulator using a trivariate Gaussian kernel. A comparison with competing techniques shows that our approach is considerably faster and scales significantly better than previous ones, being the first practical solution for deterministic plane detection in large unorganized point clouds.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\LMB42MZB\Limberger - Real-Time Detection of Planar Regions in Unorganized Point Clouds.pdf}
}

@misc{liMMRoAreMultimodal2024,
  title = {{{MMRo}}: {{Are Multimodal LLMs Eligible}} as the {{Brain}} for {{In-Home Robotics}}?},
  shorttitle = {{{MMRo}}},
  author = {Li, Jinming and Zhu, Yichen and Xu, Zhiyuan and Gu, Jindong and Zhu, Minjie and Liu, Xin and Liu, Ning and Peng, Yaxin and Feng, Feifei and Tang, Jian},
  year = {2024},
  month = jun,
  number = {arXiv:2406.19693},
  eprint = {2406.19693},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {It is fundamentally challenging for robots to serve as useful assistants in human environments because this requires addressing a spectrum of sub-problems across robotics, including perception, language understanding, reasoning, and planning. The recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated their exceptional abilities in solving complex mathematical problems, mastering commonsense and abstract reasoning. This has led to the recent utilization of MLLMs as the "brain" in robotic systems, enabling these models to conduct high-level planning prior to triggering low-level control actions for task execution. However, it remains uncertain whether existing MLLMs are reliable in serving the brain role of robots. In this study, we introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo) benchmark, which tests the capability of MLLMs for robot applications. Specifically, we identify four essential capabilities --- perception, task planning, visual reasoning, and safety measurement --- that MLLMs must possess to qualify as the robot's central processing unit. We have developed several scenarios for each capability, resulting in a total of 14 metrics for evaluation. We present experimental results for various MLLMs, including both commercial and open-source models, to assess the performance of existing systems. Our findings indicate that no single model excels in all areas, suggesting that current MLLMs are not yet trustworthy enough to serve as the cognitive core for robots. Our data can be found in https://mm-robobench.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  annotation = {titleTranslation: MMRo：多模态大语言模型是否具备成为家庭机器人大脑的资格？},
  file = {C:\Users\guaoxiang\Zotero\storage\9PXHGIDT\Li 等 - 2024 - MMRo Are Multimodal LLMs Eligible as the Brain for In-Home Robotics.pdf}
}

@article{linLeastSquaresAlgorithm2017,
  title = {A Least Squares Algorithm for Fitting Data Points to a Circular Arc Cam},
  shorttitle = {圆形凸轮拟合},
  author = {Lin, Shan and Jusko, Otto and H{\"a}rtig, Frank and Seewig, J{\"o}rg},
  year = {2017},
  month = may,
  journal = {Measurement},
  volume = {102},
  pages = {170--178},
  issn = {02632241},
  doi = {10.1016/j.measurement.2017.01.059},
  urldate = {2024-11-20},
  abstract = {Precise evaluation of form error is important for quality control in the manufacture of camshafts. For circular arc cams, a conventional method is to fit each arc segment of the cam individually. In such a case, at the connecting points of two fitted segments, there may be discontinuity or non-smoothness. In this paper, a global cam fitting algorithm based on the nonlinear least squares method is proposed. A circular arc cam is represented by the mathematic function in terms of form, rotation and position parameters. By imposing parameter constraints, a closed and smooth profile can be obtained as the result of fitting. In order to evaluate the performance of the proposed algorithm, the uncertainties of the fitted parameters are estimated by the GUM uncertainty framework and Monte Carlo simulations. Compared to the conventional cam fit, the uncertainties obtained by the proposed algorithm are lower. Additionally, the factors which significantly affect the fitting results are specified.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\DWQ9HATG\Lin 等 - 2017 - A least squares algorithm for fitting data points to a circular arc cam.pdf}
}

@article{linPlanarBasedAdaptiveSampling2016,
  title = {Planar-{{Based Adaptive Down-Sampling}} of {{Point Clouds}}},
  author = {Lin, Yun-Jou and Benziger, Ronald R and Habib, Ayman},
  year = {2016},
  month = dec,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume = {82},
  number = {12},
  pages = {955--966},
  issn = {00991112},
  doi = {10.14358/PERS.82.12.955},
  urldate = {2024-12-05},
  abstract = {Derived point clouds from laser scanners and image-based dense-matching techniques usually include tremendous number of points. Processing (e.g., segmenting) such huge dataset is time-consuming and might not be necessary. For example, a planar surface just needs few points to be defined. In contrast, linear/cylindrical and rough features require more points for reliable modeling since during the data acquisition process, only a portion of linear/cylindrical features is present in the point cloud. This paper introduces an adaptive down-sampling strategy for removing redundant points from high density planar regions while retaining points in planar areas with sparse points and all the points within linear/cylindrical and rough neighborhoods. To demonstrate the feasibility and performance of the proposed procedure, a comparison of segmentation results using original laser and image-based point clouds as well as the adaptively, uniformly, and point-spacing-based down-sampled point clouds are presented while commenting on the computational efficiency and the segmentation quality.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ETHHNUIK\Lin 等 - 2016 - Planar-Based Adaptive Down-Sampling of Point Clouds.pdf}
}

@misc{linPPTacPaperPicking2025,
  title = {{{PP-Tac}}: {{Paper Picking Using Tactile Feedback}} in {{Dexterous Robotic Hands}}},
  shorttitle = {{{PP-Tac}}},
  author = {Lin, Pei and Huang, Yuzhe and Li, Wanlin and Ma, Jianpeng and Xiao, Chenxi and Jiao, Ziyuan},
  year = {2025},
  month = jun,
  number = {arXiv:2504.16649},
  eprint = {2504.16649},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16649},
  urldate = {2025-08-07},
  abstract = {Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Although recent advances in robotic hardware and embodied AI have expanded their capabilities, current systems still struggle with handling thin, flat, and deformable objects such as paper and fabric. This limitation arises from the lack of suitable perception techniques for robust state estimation under diverse object appearances, as well as the absence of planning techniques for generating appropriate grasp motions. To bridge these gaps, this paper introduces PP-Tac, a robotic system for picking up paper-like objects. PP-Tac features a multi-fingered robotic hand with high-resolution omnidirectional tactile sensors {\textbackslash}sensorname. This hardware configuration enables real-time slip detection and online frictional force control that mitigates such slips. Furthermore, grasp motion generation is achieved through a trajectory synthesis pipeline, which first constructs a dataset of finger's pinching motions. Based on this dataset, a diffusion-based policy is trained to control the hand-arm robotic system. Experiments demonstrate that PP-Tac can effectively grasp paper-like objects of varying material, thickness, and stiffness, achieving an overall success rate of 87.5{\textbackslash}\%. To our knowledge, this work is the first attempt to grasp paper-like deformable objects using a tactile dexterous hand. Our project webpage can be found at: https://peilin-666.github.io/projects/PP-Tac/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\FIYSJTNS\\Lin 等 - 2025 - PP-Tac Paper Picking Using Tactile Feedback in Dexterous Robotic Hands.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\AMWGU9F2\\2504.html}
}

@article{liuAlgorithmResearchBased2024,
  title = {{Algorithm research based on an elliptical arc fitting curve}},
  author = {Liu, Qingjian and Li, Pei and Huang, Gangpeng and Zhang, Xu and Liu, Shuo and Yang, Ziyi and Hao, Tianze},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {113113--113125},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3416465},
  urldate = {2024-11-28},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\QFVNMQRN\Liu 等 - 2024 - Algorithm Research Based on an Elliptical Arc Fitting Curve.pdf}
}

@misc{liuAligningCyberSpace2024,
  title = {Aligning {{Cyber Space}} with {{Physical World}}: {{A Comprehensive Survey}} on {{Embodied AI}}},
  shorttitle = {Aligning {{Cyber Space}} with {{Physical World}}},
  author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  year = {2024},
  month = aug,
  number = {arXiv:2407.06886},
  eprint = {2407.06886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06886},
  urldate = {2024-12-02},
  abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github. com/HCPLab-SYSU/Embodied AI Paper List.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\QKFM78VQ\Liu 等 - 2024 - Aligning Cyber Space with Physical World A Comprehensive Survey on Embodied AI.pdf}
}

@misc{liuAligningCyberSpace2024a,
  title = {Aligning {{Cyber Space}} with {{Physical World}}: {{A Comprehensive Survey}} on {{Embodied AI}}},
  shorttitle = {Aligning {{Cyber Space}} with {{Physical World}}},
  author = {Liu, Yang and Chen, Weixing and Bai, Yongjie and Liang, Xiaodan and Li, Guanbin and Gao, Wen and Lin, Liang},
  year = {2024},
  month = aug,
  number = {arXiv:2407.06886},
  eprint = {2407.06886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06886},
  urldate = {2024-12-02},
  abstract = {Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for the brain of embodied agents. However, there is no comprehensive survey for Embodied AI in the era of MLMs. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering the state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in dynamic digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss their potential future directions. We hope this survey will serve as a foundational reference for the research community and inspire continued innovation. The associated project can be found at https://github. com/HCPLab-SYSU/Embodied AI Paper List.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\4W3ET2UM\Liu 等 - 2024 - Aligning Cyber Space with Physical World A Comprehensive Survey on Embodied AI.pdf}
}

@misc{liuLLMBasedHumanRobotCollaboration2023,
  title = {{{LLM-Based Human-Robot Collaboration Framework}} for {{Manipulation Tasks}}},
  author = {Liu, Haokun and Zhu, Yaonan and Kato, Kenji and Kondo, Izumi and Aoyama, Tadayoshi and Hasegawa, Yasuhisa},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14972},
  eprint = {2308.14972},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14972},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach to enhance autonomous robotic manipulation using the Large Language Model (LLM) for logical inference, converting high-level language commands into sequences of executable motion functions. The proposed system combines the advantage of LLM with YOLO-based environmental perception to enable robots to autonomously make reasonable decisions and task planning based on the given commands. Additionally, to address the potential inaccuracies or illogical actions arising from LLM, a combination of teleoperation and Dynamic Movement Primitives (DMP) is employed for action correction. This integration aims to improve the practicality and generalizability of the LLM-based human-robot collaboration system.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\BX5SMRQ6\\Liu 等 - 2023 - LLM-Based Human-Robot Collaboration Framework for Manipulation Tasks.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\EDJGHERZ\\2308.14972v1-zh.pdf}
}

@misc{liUniDoorManipLearningUniversal2024,
  title = {{{UniDoorManip}}: {{Learning Universal Door Manipulation Policy Over Large-scale}} and {{Diverse Door Manipulation Environments}}},
  shorttitle = {{{UniDoorManip}}},
  author = {Li, Yu and Zhang, Xiaojie and Wu, Ruihai and Zhang, Zilong and Geng, Yiran and Dong, Hao and He, Zhaofeng},
  year = {2024},
  month = mar,
  number = {arXiv:2403.02604},
  eprint = {2403.02604},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.02604},
  urldate = {2024-12-03},
  abstract = {Learning a universal manipulation policy encompassing doors with diverse categories, geometries and mechanisms, is crucial for future embodied agents to effectively work in complex and broad real-world scenarios. Due to the limited datasets and unrealistic simulation environments, previous works fail to achieve good performance across various doors. In this work, we build a novel door manipulation environment reflecting different realistic door manipulation mechanisms, and further equip this environment with a large-scale door dataset covering 6 door categories with hundreds of door bodies and handles, making up thousands of different door instances. Additionally, to better emulate real-world scenarios, we introduce a mobile robot as the agent and use the partial and occluded point cloud as the observation, which are not considered in previous works while possessing significance for real-world implementations. To learn a universal policy over diverse doors, we propose a novel framework disentangling the whole manipulation process into three stages, and integrating them by training in the reversed order of inference. Extensive experiments validate the effectiveness of our designs and demonstrate our framework's strong performance. Code, data and videos are avaible on https://unidoormanip.github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\Y8MBKEK7\Li 等 - 2024 - UniDoorManip Learning Universal Door Manipulation Policy Over Large-scale and Diverse Door Manipula.pdf}
}

@misc{liuRDT1BDiffusionFoundation2024,
  title = {{{RDT-1B}}: A {{Diffusion Foundation Model}} for {{Bimanual Manipulation}}},
  shorttitle = {{{RDT-1B}}},
  author = {Liu, Songming and Wu, Lingxuan and Li, Bangguo and Tan, Hengkai and Chen, Huayu and Wang, Zhengyi and Xu, Ke and Su, Hang and Zhu, Jun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.07864},
  eprint = {2410.07864},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zeroshot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1{$\sim$}5 demonstrations, and effectively handles complex, dexterous tasks. We refer to the project page for the code and videos.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\2VLKBVT8\\Liu 等 - 2024 - RDT-1B a Diffusion Foundation Model for Bimanual Manipulation.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\8HSGCM8T\\Liu 等 - 2024 - RDT-1B a Diffusion Foundation Model for Bimanual Manipulation.pdf}
}

@misc{liuTTFVLATemporalToken2025,
  title = {{{TTF-VLA}}: {{Temporal Token Fusion}} via {{Pixel-Attention Integration}} for {{Vision-Language-Action Models}}},
  shorttitle = {{{TTF-VLA}}},
  author = {Liu, Chenghao and Zhang, Jiachen and Li, Chengxuan and Zhou, Zhimu and Wu, Shixin and Huang, Songfang and Duan, Huiling},
  year = {2025},
  month = aug,
  number = {arXiv:2508.19257},
  eprint = {2508.19257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.19257},
  urldate = {2025-09-16},
  abstract = {Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4{\textbackslash}\% vs 68.4{\textbackslash}\% baseline), cross-environment validation on SimplerEnv (4.8{\textbackslash}\% relative improvement), and 8.7{\textbackslash}\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\93ISUEEE\\Liu 等 - 2025 - TTF-VLA Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\KVVIQA95\\Liu 等 - 2025 - TTF-VLA Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\VHG8T5FV\\Liu 等 - 2025 - TTF-VLA Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\8HYRKYAF\\2508.html}
}

@article{LiuZiHuiSanWeiShiYuSanBanDongTaiGanSheCeLiangXiTongDeJiChengHuaSheJi2020,
  title = {三维时域散斑动态干涉测量系统的集成化设计},
  author = {刘子惠 and 高瞻 and 高晨家 and 王煦 and 钟楚千 and 刘宇琛 and 张园},
  year = {2020},
  journal = {Journal of Applied Optics},
  volume = {41},
  number = {4},
  pages = {829--836},
  publisher = {应用光学编辑部}
}

@misc{lykovLLMBRAInAIdrivenFast2023,
  title = {{{LLM-BRAIn}}: {{AI-driven Fast Generation}} of {{Robot Behaviour Tree}} Based on {{Large Language Model}}},
  shorttitle = {{{LLM-BRAIn}}},
  author = {Lykov, Artem and Tsetserukou, Dzmitry},
  year = {2023},
  month = may,
  number = {arXiv:2305.19352},
  eprint = {2305.19352},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {This paper presents a novel approach in autonomous robot control, named LLMBRAIn, that makes possible robot behavior generation, based on operator's commands. LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description. We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003. The developed model accurately builds complex robot behavior while remaining small enough to be run on the robot's onboard microcomputer. The model gives structural and logical correct BTs and can successfully manage instructions that were not presented in training set. The experiment did not reveal any significant subjective differences between BTs generated by LLM-BRAIn and those created by humans (on average, participants were able to correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in only 4.53 out of 10 cases, indicating that their performance was close to random chance). The proposed approach potentially can be applied to mobile robotics, drone operation, robot manipulator systems and Industry 4.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\IIXVTEBS\\Lykov和Tsetserukou - 2023 - LLM-BRAIn AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\SLXM96A9\\2305.19352v1 (1)-zh.pdf}
}

@misc{lykovLLMBRAInAIdrivenFast2023a,
  title = {{{LLM-BRAIn}}: {{AI-driven Fast Generation}} of {{Robot Behaviour Tree}} Based on {{Large Language Model}}},
  shorttitle = {{{LLM-BRAIn}}},
  author = {Lykov, Artem and Tsetserukou, Dzmitry},
  year = {2023},
  month = may,
  number = {arXiv:2305.19352},
  eprint = {2305.19352},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.19352},
  urldate = {2024-12-01},
  abstract = {This paper presents a novel approach in autonomous robot control, named LLMBRAIn, that makes possible robot behavior generation, based on operator's commands. LLM-BRAIn is a transformer-based Large Language Model (LLM) finetuned from Stanford Alpaca 7B model to generate robot behavior tree (BT) from the text description. We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003. The developed model accurately builds complex robot behavior while remaining small enough to be run on the robot's onboard microcomputer. The model gives structural and logical correct BTs and can successfully manage instructions that were not presented in training set. The experiment did not reveal any significant subjective differences between BTs generated by LLM-BRAIn and those created by humans (on average, participants were able to correctly distinguish between LLM-BRAIn generated BTs and human-created BTs in only 4.53 out of 10 cases, indicating that their performance was close to random chance). The proposed approach potentially can be applied to mobile robotics, drone operation, robot manipulator systems and Industry 4.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\5Y6PLRVC\Lykov和Tsetserukou - 2023 - LLM-BRAIn AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model.pdf}
}

@article{maiLLMRoboticBrain,
  title = {{{LLM}} as {{A Robotic Brain}}: {{Unifying Egocentric Memory}} and {{Control}}},
  author = {Mai, Jinjie and Chen, Jun and Li, Bing and Qian, Guocheng and Elhoseiny, Mohamed and Ghanem, Bernard},
  abstract = {Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLMBrain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\LPYGFKP3\Mai 等 - LLM as A Robotic Brain Unifying Egocentric Memory and Control.pdf}
}

@misc{majumdarImprovingVisionLanguageNavigation2020,
  title = {Improving {{Vision-and-Language Navigation}} with {{Image-Text Pairs}} from the {{Web}}},
  author = {Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = may,
  number = {arXiv:2004.14973},
  eprint = {2004.14973},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Following a navigation instruction such as `Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground scene elements referenced via language (e.g.`stairs') to visual content in the environment (pixels corresponding to `stairs').},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\IAK6CMND\2004.14973v2.pdf}
}

@book{MeisheldonrossZhu;ZhaoXuanMinDengYi.GaiLuLunJiChuJiaoCheng2006,
  title = {{概率论基础教程}},
  author = {{(美)Sheldon Ross著 ; 赵选民等译.} and {罗斯.} and {赵选民.}},
  year = {2006},
  publisher = {机械工业出版社},
  address = {Bei jing},
  abstract = {本书系统介绍了概率论的基础理论及应用, 主要内容包括:组合分析, 概率论的公理, 条件概率与独立性, 随机变量及其分布, 数学期望, 极限定理, 随机模拟等},
  isbn = {978-7-111-18378-5},
  langid = {chinese},
  annotation = {OCLC: 303906169},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\I38BHNH5\\(美)Sheldon Ross著 \; 赵选民等译. 等 - 2006 - 概率论基础教程.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\J279ZP9E\\a_first_course_in_probability.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\MKFYM42Y\\A_Second_Course_in_Probability-Ross-Pekoz.pdf}
}

@article{melzerSVDItsApplication,
  title = {{{SVD}} and Its {{Application}} to {{Generalized Eigenvalue Problems}}},
  author = {Melzer, Thomas},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\8S7BL4FE\\SVD_EVD.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\NZU7BXFS\\Melzer - SVD and its Application to Generalized Eigenvalue Problems.pdf}
}

@article{mihaylovnaCYLINDRICALFUNCTIONSTHEIR,
  title = {{{CYLINDRICAL FUNCTIONS AND THEIR APPLICATIONS TO SOLVING PROBLEMS OF MATHEMATICAL PHYSICS}}},
  author = {Mihaylovna, Bogdan Anna},
  abstract = {The article is devoted to the study of cylindrical functions and their application in problems of mathematical physics. Cylindrical functions are widely used in modeling physical processes in cylindrical regions, such as wave propagation, thermal conductivity, and electromagnetic fields. The paper discusses the main types of cylindrical functions, including Bessel functions, modified Bessel functions and other special functions. The differential equations defining cylindrical functions and their basic properties are studied. Examples are given of the use of cylindrical functions to solve the Laplace equation, the Helmholtz equation and the heat equation in cylindrical coordinates. Particular attention is paid to the issues of numerical modeling of cylindrical functions and the features of their implementation in software. Thus, this work represents a comprehensive study of cylindrical functions and their use in problems of mathematical physics.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\5CHX77JD\Mihaylovna - CYLINDRICAL FUNCTIONS AND THEIR APPLICATIONS TO SOLVING PROBLEMS OF MATHEMATICAL PHYSICS.pdf}
}

@misc{moWhere2ActPixelsActions2021,
  title = {{{Where2Act}}: {{From Pixels}} to {{Actions}} for {{Articulated 3D Objects}}},
  shorttitle = {{{Where2Act}}},
  author = {Mo, Kaichun and Guibas, Leonidas and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
  year = {2021},
  month = aug,
  number = {arXiv:2101.02692},
  eprint = {2101.02692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.02692},
  urldate = {2025-08-11},
  abstract = {One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal --we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. Check the website for code and data release.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\4W3NLFQQ\\Mo 等 - 2021 - Where2Act From Pixels to Actions for Articulated 3D Objects.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\SPIBQLUH\\Mo 等 - 2021 - Where2Act From Pixels to Actions for Articulated 3D Objects.pdf}
}

@misc{moWhere2ActPixelsActions2021a,
  title = {{Where2Act: From Pixels to Actions for Articulated 3D Objects}},
  shorttitle = {{Where2Act}},
  author = {Mo, Kaichun and Guibas, Leonidas and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
  year = {2021},
  month = aug,
  number = {arXiv:2101.02692},
  eprint = {2101.02692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.02692},
  urldate = {2025-08-11},
  abstract = {One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal -- we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. Check the website for code and data release: https://cs.stanford.edu/{\textasciitilde}kaichun/where2act/},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics}
}

@misc{muRoboCodeXMultimodalCode2024,
  title = {{{RoboCodeX}}: {{Multimodal Code Generation}} for {{Robotic Behavior Synthesis}}},
  shorttitle = {{{RoboCodeX}}},
  author = {Mu, Yao and Chen, Junting and Zhang, Qinglong and Chen, Shoufa and Yu, Qiaojun and Ge, Chongjian and Chen, Runjian and Liang, Zhixuan and Hu, Mengkang and Tao, Chaofan and Sun, Peize and Yu, Haibao and Yang, Chao and Shao, Wenqi and Wang, Wenhai and Dai, Jifeng and Qiao, Yu and Ding, Mingyu and Luo, Ping},
  year = {2024},
  month = feb,
  number = {arXiv:2402.16117},
  eprint = {2402.16117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16117},
  urldate = {2025-09-14},
  abstract = {Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\3ZFC82V7\Mu et al. - 2024 - RoboCodeX: Multimodal Code Generation for Robotic.pdf}
}

@misc{muRoboCodeXMultimodalCode2024a,
  title = {{{RoboCodeX}}: {{Multimodal Code Generation}} for {{Robotic Behavior Synthesis}}},
  shorttitle = {{{RoboCodeX}}},
  author = {Mu, Yao and Chen, Junting and Zhang, Qinglong and Chen, Shoufa and Yu, Qiaojun and Ge, Chongjian and Chen, Runjian and Liang, Zhixuan and Hu, Mengkang and Tao, Chaofan and Sun, Peize and Yu, Haibao and Yang, Chao and Shao, Wenqi and Wang, Wenhai and Dai, Jifeng and Qiao, Yu and Ding, Mingyu and Luo, Ping},
  year = {2024},
  month = feb,
  number = {arXiv:2402.16117},
  eprint = {2402.16117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16117},
  urldate = {2025-09-14},
  abstract = {Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.},
  archiveprefix = {arXiv},
  keywords = {3D,code,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\27DMTED8\\Mu 等 - 2024 - RoboCodeX Multimodal Code Generation for Robotic Behavior Synthesis.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\H3LR6EC9\\Mu 等 - 2024 - RoboCodeX Multimodal Code Generation for Robotic Behavior Synthesis.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\LS9I6BI7\\Mu 等 - 2024 - RoboCodeX Multimodal Code Generation for Robotic Behavior Synthesis.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\NIKKLJCR\\2402.html}
}

@misc{NeRFNeuralRadiance,
  title = {{{NeRF}}: {{Neural Radiance Fields}}},
  shorttitle = {{{NeRF}}},
  urldate = {2025-08-11},
  abstract = {A method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views.},
  howpublished = {https://www.matthewtancik.com/nerf},
  keywords = {,nerf},
  file = {C:\Users\guaoxiang\Zotero\storage\8TKVMXBT\nerf.html}
}

@inproceedings{NIPS2017_d8bf84be,
  title = {{{PointNet}}++: {{Deep}} Hierarchical Feature Learning on Point Sets in a Metric Space},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.}
}

@article{nurunnabiRobustCylinderFitting,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1 m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84 m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1 m with only 0.27{$^\circ$} bias angle in the principal axis.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\AHWETT86\Nurunnabi - Robust cylinder fitting in laser scanning point cloud data.pdf}
}

@article{nurunnabiRobustCylinderFitting2019,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul and Sadahiro, Yukio and Lindenbergh, Roderik and Belton, David},
  year = {2019},
  month = may,
  journal = {Measurement},
  volume = {138},
  pages = {632--651},
  issn = {0263-2241},
  doi = {10.1016/j.measurement.2019.01.095},
  urldate = {2024-11-30},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1\,m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84\,m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1\,m with only 0.27{$^\circ$} bias angle in the principal axis.},
  keywords = {3D modelling,Feature extraction,Robust measurement,Robust PCA,Robust regression,Shape reconstruction},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\W2IEMDV3\\Nurunnabi 等 - 2019 - Robust cylinder fitting in laser scanning point cloud data.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\EGYMY7W7\\S0263224119301046.html}
}

@article{nurunnabiRobustCylinderFittinga,
  title = {Robust Cylinder Fitting in Laser Scanning Point Cloud Data},
  author = {Nurunnabi, Abdul},
  abstract = {Cylinders play a vital role in representing geometry of environmental and man-made structures. Most existing cylinder fitting methods perform well for outlier free data sampling a full cylinder, but are not reliable in the presence of outliers or incomplete data. Point Cloud Data (PCD) are typically outlier contaminated and incomplete. This paper presents two robust cylinder fitting algorithms for PCD that use robust Principal Component Analysis (PCA) and robust regression. Experiments with simulated and real data show that the new methods are efficient (i) in the presence of outliers, (ii) for partially and fully sampled cylinders, (iii) for small and large numbers of points, (iv) for various sizes: radii and lengths, and (v) for cylinders with unequal radii at their ends. A simulation study consisting of 1000 cylinders of 1 m radius with 20\% clustered outliers, reveals that a PCA based method fits cylinders with an average radius of 2.84 m and with a principal axis biased by outliers of 9.65{$^\circ$} on average, whereas the proposed robust method correctly estimates the average radius of 1 m with only 0.27{$^\circ$} bias angle in the principal axis.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\MJW9LUEV\Nurunnabi - Robust cylinder fitting in laser scanning point cloud data.pdf}
}

@article{nurunnabiRobustStatisticalApproaches2014,
  title = {Robust Statistical Approaches for Local Planar Surface Fitting in {{3D}} Laser Scanning Data},
  author = {Nurunnabi, Abdul and Belton, David and West, Geoff},
  year = {2014},
  month = oct,
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {96},
  pages = {106--122},
  issn = {09242716},
  doi = {10.1016/j.isprsjprs.2014.07.004},
  urldate = {2024-12-28},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\BB53UCMZ\Nurunnabi 等 - 2014 - Robust statistical approaches for local planar surface fitting in 3D laser scanning data.pdf}
}

@misc{PDFREDEEndEnd,
  title = {[{{PDF}}] {{REDE}}: {{End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination}} {\textbar} {{Semantic Scholar}}},
  urldate = {2024-11-01},
  howpublished = {https://www.semanticscholar.org/paper/REDE\%3A-End-to-End-Object-6D-Pose-Robust-Estimation-Hua-Zhou/20f751603254a524c9b2de6c7076474e2260b945},
  keywords = {Pose},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\TQU6QK9R\\[PDF] REDE End-to-End Object 6D Pose Robust Estimation Using Differentiable Outliers Elimination .pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\WSPJGHV3\\20f751603254a524c9b2de6c7076474e2260b945.html}
}

@misc{peeblesScalableDiffusionModels2023,
  title = {Scalable {{Diffusion Models}} with {{Transformers}}},
  author = {Peebles, William and Xie, Saining},
  year = {2023},
  month = mar,
  number = {arXiv:2212.09748},
  eprint = {2212.09748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09748},
  urldate = {2025-09-16},
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\B52DCFSV\Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf}
}

@misc{peeblesScalableDiffusionModels2023a,
  title = {Scalable {{Diffusion Models}} with {{Transformers}}},
  author = {Peebles, William and Xie, Saining},
  year = {2023},
  month = mar,
  number = {arXiv:2212.09748},
  eprint = {2212.09748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09748},
  urldate = {2025-09-16},
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\LRGE3AJ9\\Peebles和Xie - 2023 - Scalable Diffusion Models with Transformers.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\R9LYNZYF\\2212.html}
}

@misc{pertschFASTEfficientAction2025,
  title = {{{FAST}}: {{Efficient Action Tokenization}} for {{Vision-Language-Action Models}}},
  shorttitle = {{{FAST}}},
  author = {Pertsch, Karl and Stachowicz, Kyle and Ichter, Brian and Driess, Danny and Nair, Suraj and Vuong, Quan and Mees, Oier and Finn, Chelsea and Levine, Sergey},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09747},
  eprint = {2501.09747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09747},
  urldate = {2025-08-22},
  abstract = {Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the {$\pi$}0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\PP8RK5LA\Pertsch 等 - 2025 - FAST Efficient Action Tokenization for Vision-Language-Action Models.pdf}
}

@article{petersenHttpMatrixcookbookcom,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\TNN8BLHN\Petersen和Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{petersenHttpMatrixcookbookcoma,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\6LG6R8QZ\Petersen和Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{petersenHttpMatrixcookbookcomb,
  title = {[ {{http://matrixcookbook.com}} ]},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  langid = {english}
}

@article{prasadElliFitUnconstrainedNoniterative2013,
  title = {{ElliFit: An unconstrained, non-iterative, least squares based geometric Ellipse Fitting method}},
  shorttitle = {{ElliFit}},
  author = {Prasad, Dilip K. and Leung, Maylor K.H. and Quek, Chai},
  year = {2013},
  month = may,
  journal = {Pattern Recognition},
  volume = {46},
  number = {5},
  pages = {1449--1465},
  issn = {00313203},
  doi = {10.1016/j.patcog.2012.11.007},
  urldate = {2024-11-26},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\VVU957RS\Prasad 等 - 2013 - ElliFit An unconstrained, non-iterative, least squares based geometric Ellipse Fitting method.pdf}
}

@incollection{prasadPreciseEllipseFitting2012,
  title = {A {{Precise Ellipse Fitting Method}} for {{Noisy Data}}},
  booktitle = {Image {{Analysis}} and {{Recognition}}},
  author = {Prasad, Dilip K. and Quek, Chai and Leung, Maylor K. H.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Campilho, Aur{\'e}lio and Kamel, Mohamed},
  year = {2012},
  volume = {7324},
  pages = {253--260},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-31295-3_30},
  urldate = {2024-11-28},
  abstract = {Least squares based ellipse detection is used as a core process in many image processing applications. In order to restrict the solution to ellipses and avoid non-elliptic conics, constrained optimization has to be incorporated in the least squares model. This paper proposes a least squares method that does not require a constrained optimization and has very low false positive rates. In contrast to the algebraic model of conics, we use the geometric model of ellipse and minimize the geometric distance of the fitted ellipse from the digital curve. As a result, the solutions are strictly restricted to ellipses. Results demonstrate a superior performance than most least squares based method for elliptic curves and greater true negative rates for non-elliptic curves even in presence of 30\% noise.},
  isbn = {978-3-642-31294-6 978-3-642-31295-3},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\V3KN7R2S\\translated_A-Precise-Ellipse-Fitting-Method-for-Noisy-Data.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\YYMTYA8X\\Prasad 等 - 2012 - A Precise Ellipse Fitting Method for Noisy Data.pdf}
}

@misc{puigVirtualHomeSimulatingHousehold2018,
  title = {{{VirtualHome}}: {{Simulating Household Activities}} via {{Programs}}},
  shorttitle = {{{VirtualHome}}},
  author = {Puig, Xavier and Ra, Kevin and Boben, Marko and Li, Jiaman and Wang, Tingwu and Fidler, Sanja and Torralba, Antonio},
  year = {2018},
  month = jun,
  number = {arXiv:1806.07011},
  eprint = {1806.07011},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos. We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to ``drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language descriptions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\guaoxiang\Zotero\storage\KW24T4NN\Puig 等 - 2018 - VirtualHome Simulating Household Activities via Programs.pdf}
}

@inproceedings{qiPointNetDeepHierarchical2017,
  title = {{{PointNet}}++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
  shorttitle = {{{PointNet}}++},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {5105--5114},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-09-13},
  abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\guaoxiang\Zotero\storage\JSMUUPGA\Qi et al. - 2017 - PointNet++: deep hierarchical feature learning on.pdf}
}

@article{rabinerIntroductionHiddenMarkov1986,
  title = {An Introduction to Hidden {{Markov}} Models},
  author = {Rabiner, L. and Juang, B.},
  year = {1986},
  journal = {IEEE ASSP Magazine},
  volume = {3},
  number = {1},
  pages = {4--16},
  issn = {0740-7467},
  doi = {10.1109/MASSP.1986.1165342},
  urldate = {2025-06-17},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\JPEI4VLP\Rabiner和Juang - 1986 - An introduction to hidden Markov models.pdf}
}

@misc{radfordRobustSpeechRecognition2022,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04356},
  eprint = {2212.04356},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.04356},
  urldate = {2024-12-03},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\guaoxiang\Zotero\storage\HWJ84ZZK\Radford 等 - 2022 - Robust Speech Recognition via Large-Scale Weak Supervision.pdf}
}

@article{radIntroductionDockerAnalysis2017,
  title = {An Introduction to Docker and Analysis of Its Performance},
  author = {Rad, Babak Bashari and Bhatti, Harrison John and Ahmadi, Mohammad},
  year = {2017},
  journal = {International Journal of Computer Science and Network Security (IJCSNS)},
  volume = {17},
  number = {3},
  pages = {228},
  publisher = {{International Journal of Computer Science and Network Security}},
  isbn = {1738-7906}
}

@article{ranaSayPlanGroundingLarge,
  title = {{{SayPlan}}: {{Grounding Large Language Models}} Using {{3D Scene Graphs}} for {{Scalable Robot Task Planning}}},
  author = {Rana, Krishan and Haviland, Jesse and Garg, Sourav and {Abou-Chakra}, Jad and Reid, Ian},
  abstract = {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page sayplan.github.io.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\M4UK2RU8\Rana 等 - SayPlan Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning.pdf}
}

@article{ribeiroKalmanExtendedKalman,
  title = {Kalman and {{Extended Kalman Filters}}: {{Concept}}, {{Derivation}} and {{Properties}}},
  author = {Ribeiro, Maria Isabel},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ZZJY4F5R\Ribeiro - Kalman and Extended Kalman Filters Concept, Derivation and Properties.pdf}
}

@book{rossIntroductionProbabilityModels2010,
  title = {{Introduction to probability models}},
  author = {Ross, Sheldon M.},
  year = {2010},
  edition = {Tenth edition},
  publisher = {Academic Press, an imprint of Elsevier},
  address = {Amsterdam Boston},
  isbn = {978-0-12-375686-2 978-0-12-375687-9},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\VZHQNUML\Ross - 2010 - Introduction to probability models.pdf}
}

@misc{shahLMNavRoboticNavigation2022,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  author = {Shah, Dhruv and Osinski, Blazej and Ichter, Brian and Levine, Sergey},
  year = {2022},
  month = jul,
  number = {arXiv:2207.04429},
  eprint = {2207.04429},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. We instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\FRXT5V7X\Shah 等 - 2022 - LM-Nav Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.pdf}
}

@article{shakarjiLeastsquaresFittingAlgorithms1998,
  title = {Least-Squares Fitting Algorithms of the {{NIST}} Algorithm Testing System},
  author = {Shakarji, C.M.},
  year = {1998},
  month = nov,
  journal = {Journal of Research of the National Institute of Standards and Technology},
  volume = {103},
  number = {6},
  pages = {633},
  issn = {1044677X},
  doi = {10.6028/jres.103.043},
  urldate = {2024-11-08},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\ME9FHGNG\Shakarji - 1998 - Least-squares fitting algorithms of the NIST algorithm testing system.pdf}
}

@misc{shiDiversityAllYou2025,
  title = {Is {{Diversity All You Need}} for {{Scalable Robotic Manipulation}}?},
  author = {Shi, Modi and Chen, Li and Chen, Jin and Lu, Yuxiang and Liu, Chiming and Ren, Guanghui and Luo, Ping and Huang, Di and Yao, Maoqing and Li, Hongyang},
  year = {2025},
  month = jul,
  number = {arXiv:2507.06219},
  eprint = {2507.06219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.06219},
  urldate = {2025-09-16},
  abstract = {Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15\%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\QIX2ZX5Q\\Shi et al. - 2025 - Is Diversity All You Need for Scalable Robotic Man.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\WR77WWNZ\\Shi et al. - 2025 - Is Diversity All You Need for Scalable Robotic Man.html}
}

@misc{shiDiversityAllYou2025a,
  title = {Is {{Diversity All You Need}} for {{Scalable Robotic Manipulation}}?},
  author = {Shi, Modi and Chen, Li and Chen, Jin and Lu, Yuxiang and Liu, Chiming and Ren, Guanghui and Luo, Ping and Huang, Di and Yao, Maoqing and Li, Hongyang},
  year = {2025},
  month = jul,
  number = {arXiv:2507.06219},
  eprint = {2507.06219},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.06219},
  urldate = {2025-09-16},
  abstract = {Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15\%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\FT2365XY\\Shi 等 - 2025 - Is Diversity All You Need for Scalable Robotic Manipulation.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\K5697AIW\\Shi 等 - 2025 - Is Diversity All You Need for Scalable Robotic Manipulation.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\TYFYAXXF\\2507.html}
}

@misc{shiMemoryVLAPerceptualCognitiveMemory2025,
  title = {{{MemoryVLA}}: {{Perceptual-Cognitive Memory}} in {{Vision-Language-Action Models}} for {{Robotic Manipulation}}},
  shorttitle = {{{MemoryVLA}}},
  author = {Shi, Hao and Xie, Bin and Liu, Yingfei and Sun, Lin and Liu, Fengrong and Wang, Tiancai and Zhou, Erjin and Fan, Haoqiang and Zhang, Xiangyu and Huang, Gao},
  year = {2025},
  month = aug,
  number = {arXiv:2508.19236},
  eprint = {2508.19236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.19236},
  urldate = {2025-09-16},
  abstract = {Temporal context is essential for robotic manipulation because such tasks are inherently non-Markovian, yet mainstream VLA models typically overlook it and struggle with long-horizon, temporally dependent tasks. Cognitive science suggests that humans rely on working memory to buffer short-lived representations for immediate control, while the hippocampal system preserves verbatim episodic details and semantic gist of past experience for long-term memory. Inspired by these mechanisms, we propose MemoryVLA, a Cognition-Memory-Action framework for long-horizon robotic manipulation. A pretrained VLM encodes the observation into perceptual and cognitive tokens that form working memory, while a Perceptual-Cognitive Memory Bank stores low-level details and high-level semantics consolidated from it. Working memory retrieves decision-relevant entries from the bank, adaptively fuses them with current tokens, and updates the bank by merging redundancies. Using these tokens, a memory-conditioned diffusion action expert yields temporally aware action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it achieves 71.9\%, 72.7\%, and 96.5\% success rates, respectively, all outperforming state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on Bridge. On 12 real-world tasks spanning general skills and long-horizon temporal dependencies, MemoryVLA achieves 84.0\% success rate, with long-horizon tasks showing a +26 improvement over state-of-the-art baseline. Project Page: https://shihao1895.github.io/MemoryVLA},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\MEYZ7BPR\\Shi 等 - 2025 - MemoryVLA Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\TWT3BAQW\\Shi 等 - 2025 - MemoryVLA Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\C2FRU3EW\\2508.html}
}

@misc{SoraFoundationRobots,
  title = {Sora for Foundation Robots with Parallel Intelligence: Three World Models, Three Robotic Systems {\textbar} {{Frontiers}} of {{Information Technology}} \& {{Electronic Engineering}}},
  urldate = {2024-11-17},
  howpublished = {https://link.springer.com/article/10.1631/FITEE.2400144},
  keywords = {sora},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\6EMXWWYP\\Sora for foundation robots with parallel intelligence three world models, three robotic systems  F.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\66HLZCE2\\FITEE.html}
}

@article{stoicaChordScalablePeerpeer,
  title = {Chord: {{A Scalable Peer-to-peer Lookup Service}} for {{Internet Applications}}},
  author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M Frans and Balakrishnan, Hari},
  abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\829W7569\Stoica 等 - Chord A Scalable Peer-to-peer Lookup Service for Internet Applications.pdf}
}

@article{stoicaChordScalablePeerpeera,
  title = {Chord: {{A Scalable Peer-to-peer Lookup Service}} for {{Internet Applications}}},
  author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M Frans and Balakrishnan, Hari},
  abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\2JXXVR3E\Stoica 等 - Chord A Scalable Peer-to-peer Lookup Service for Internet Applications.pdf}
}

@article{strangArtLinearAlgebra2024,
  title = {{The Art of Linear Algebra}},
  author = {Strang, Gilbert and Hiranabe, Kenji and Fernandes, Ashley},
  year = {2024},
  month = mar,
  journal = {PRIMUS},
  pages = {1--14},
  issn = {1051-1970, 1935-4053},
  doi = {10.1080/10511970.2024.2321349},
  urldate = {2024-11-09},
  abstract = {我尝试为 Gilbert Strang 在书籍``Linear Algebra for Everyone''中介绍的矩阵的重要概念进行可视化图 释, 以促进从矩阵分解的角度对向量、矩阵计算和算法的理解. 1 它们包括矩阵分解 (Column-Row, CR)、高 斯消去法 (Gaussian Elimination, LU )、格拉姆-施密特正交化 (Gram-Schmidt Orthogonalization, QR)、特 征值和对角化 (Eigenvalues and Diagonalization, Q{$\Lambda$}QT)、和奇异值分解 (Singular Value Decomposition, U {$\Sigma$}V T).},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\2TSUQVIV\Strang 等 - 2024 - The Art of Linear Algebra.pdf}
}

@article{strangLinearAlgebraIts,
  title = {Linear {{Algebra}} and {{Its Applications}}},
  author = {Strang, Gilbert},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\667AGVU5\Strang - Linear Algebra and Its Applications.pdf}
}

@article{SymPyDocumentation,
  title = {{{SymPy Documentation}}},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\752UU7SA\SymPy Documentation.pdf}
}

@article{taoNewFittingMethod2018,
  title = {A New Fitting Method for Measurement of the Curvature Radius of a Short Arc with High Precision},
  author = {Tao, Wei and Zhong, Hong and Chen, Xiao and Selami, Yassine and Zhao, Hui},
  year = {2018},
  journal = {Measurement Science and Technology},
  volume = {29},
  number = {7},
  pages = {075014},
  publisher = {IOP Publishing},
  isbn = {0957-0233}
}

@misc{teamOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  year = {2024},
  month = may,
  number = {arXiv:2405.12213},
  eprint = {2405.12213},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\5383YKA9\Team 等 - 2024 - Octo An Open-Source Generalist Robot Policy.pdf}
}

@misc{tianRoboKeyGenRobotPose2024,
  title = {{{RoboKeyGen}}: {{Robot Pose}} and {{Joint Angles Estimation}} via {{Diffusion-based 3D Keypoint Generation}}},
  shorttitle = {{{RoboKeyGen}}},
  author = {Tian, Yang and Zhang, Jiyao and Huang, Guowei and Wang, Bin and Wang, Ping and Pang, Jiangmiao and Dong, Hao},
  year = {2024},
  month = mar,
  number = {arXiv:2403.18259},
  eprint = {2403.18259},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Estimating robot pose and joint angles is significant in advanced robotics, enabling applications like robot collaboration and online hand-eye calibration. However, the introduction of unknown joint angles makes prediction more complex than simple robot pose estimation, due to its higher dimensionality. Previous methods either regress 3D keypoints directly or utilise a render\&compare strategy. These approaches often falter in terms of performance or efficiency and grapple with the cross-camera gap problem. This paper presents a novel framework that bifurcates the high-dimensional prediction task into two manageable subtasks: 2D keypoints detection and lifting 2D keypoints to 3D. This separation promises enhanced performance without sacrificing the efficiency innate to keypoint-based techniques. A vital component of our method is the lifting of 2D keypoints to 3D keypoints. Common deterministic regression methods may falter when faced with uncertainties from 2D detection errors or self-occlusions. Leveraging the robust modeling potential of diffusion models, we reframe this issue as a conditional 3D keypoints generation task. To bolster cross-camera adaptability, we introduce the Normalised Camera Coordinate Space (NCCS), ensuring alignment of estimated 2D keypoints across varying camera intrinsics. Experimental results demonstrate that the proposed method outperforms the state-of-the-art render\&compare method and achieves higher inference speed. Furthermore, the tests accentuate our method's robust cross-camera generalisation capabilities. We intend to release both the dataset and code in https://nimolty.github.io/Robokeygen/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\QMVHIEZ5\Tian 等 - 2024 - RoboKeyGen Robot Pose and Joint Angles Estimation via Diffusion-based 3D Keypoint Generation.pdf}
}

@misc{touraniVisualSLAMWhat2022,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10491},
  eprint = {2210.10491},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of forty-five impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\J4ZSVYR4\\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\NCI8ZZCC\\2210.html}
}

@article{touraniVisualSLAMWhat2022a,
  title = {Visual {{SLAM}}: {{What}} Are the {{Current Trends}} and {{What}} to {{Expect}}?},
  shorttitle = {Visual {{SLAM}}},
  author = {Tourani, Ali and Bavle, Hriday and {Sanchez-Lopez}, Jose Luis and Voos, Holger},
  year = {2022},
  month = nov,
  journal = {Sensors},
  volume = {22},
  number = {23},
  eprint = {2210.10491},
  primaryclass = {cs},
  pages = {9297},
  issn = {1424-8220},
  doi = {10.3390/s22239297},
  urldate = {2024-11-06},
  abstract = {Vision-based sensors have shown significant performance, accuracy, and efficiency gain in Simultaneous Localization and Mapping (SLAM) systems in recent years. In this regard, Visual Simultaneous Localization and Mapping (VSLAM) methods refer to the SLAM approaches that employ cameras for pose estimation and map generation. We can see many research works that demonstrated VSLAMs can outperform traditional methods, which rely only on a particular sensor, such as a Lidar, even with lower costs. VSLAM approaches utilize different camera types (e.g., monocular, stereo, and RGB-D), have been tested on various datasets (e.g., KITTI, TUM RGB-D, and EuRoC) and in dissimilar environments (e.g., indoors and outdoors), and employ multiple algorithms and methodologies to have a better understanding of the environment. The mentioned variations have made this topic popular for researchers and resulted in a wide range of VSLAMs methodologies. In this regard, the primary intent of this survey is to present the recent advances in VSLAM systems, along with discussing the existing challenges and trends. We have given an in-depth literature survey of fortyfive impactful papers published in the domain of VSLAMs. We have classified these manuscripts by different characteristics, including the novelty domain, objectives, employed algorithms, and semantic level. We also discuss the current trends and future directions that may help researchers investigate them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\K574BRR3\Tourani 等 - 2022 - Visual SLAM What are the Current Trends and What to Expect.pdf}
}

@misc{TutorialGraphBasedSLAM,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-10-30},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\Z5EKTX2J\5681215.html}
}

@misc{TutorialGraphBasedSLAMa,
  title = {A {{Tutorial}} on {{Graph-Based SLAM}}},
  urldate = {2024-10-30},
  abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as GPS. This so-called simultaneous localization and mapping (SLAM) problem has been one of the most popular research topics in mobile robotics for the last two decades and efficient approaches for solving this task have been proposed. One intuitive way of formulating SLAM is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by finding the spatial configuration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based SLAM problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the SLAM problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/5681215},
  langid = {american},
  keywords = {SLAM},
  file = {C:\Users\guaoxiang\Zotero\storage\GQ3HB9FH\A Tutorial on Graph-Based SLAM.pdf}
}

@misc{vempralaChatGPTRoboticsDesign2023,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\G4XHY63Q\\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\6M95KQZT\\2306.html}
}

@misc{vempralaChatGPTRoboticsDesign2023a,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\9JWJV8QA\Vemprala 等 - 2023 - ChatGPT for Robotics Design Principles and Model Abilities.pdf}
}

@article{WANG20032953,
  title = {Using Symmetry in Robust Model Fitting},
  author = {Wang, Hanzi and Suter, David},
  year = {2003},
  journal = {Pattern Recognition Letters},
  volume = {24},
  number = {16},
  pages = {2953--2966},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(03)00156-9},
  abstract = {The pattern recognition and computer vision communities often employ robust methods for model fitting. In particular, high breakdown-point methods such as least median of squares (LMedS) and least trimmed squares (LTS) have often been used in situations where the data are contaminated with outliers. However, though the breakdown point of these methods can be as high as 50},
  keywords = {Breakdown point,Clustered outliers,Robust regression,Symmetry distance}
}

@misc{wangScalingProprioceptiveVisualLearning2024,
  title = {Scaling {{Proprioceptive-Visual Learning}} with {{Heterogeneous Pre-trained Transformers}}},
  author = {Wang, Lirui and Chen, Xinlei and Zhao, Jialiang and He, Kaiming},
  year = {2024},
  month = sep,
  number = {arXiv:2409.20537},
  eprint = {2409.20537},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.20537},
  urldate = {2024-12-03},
  abstract = {One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pretraining on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20\% on unseen tasks in multiple simulator benchmarks and real-world settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\AM9IAWAL\Wang 等 - 2024 - Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers.pdf}
}

@article{wangTemporalElectronicSpeckle2018,
  title = {Temporal Electronic Speckle Pattern Interferometry for Real-Time in-Plane Rotation Analysis},
  author = {Wang, Shengjia and Lu, Min and Bilgeri, Laura Maria and Jakobi, Martin and Bloise, F{\'e}lix Salazar and Koch, Alexander W},
  year = {2018},
  journal = {Optics Express},
  volume = {26},
  number = {7},
  pages = {8744--8755},
  publisher = {Optical Society of America},
  issn = {1094-4087}
}

@article{wangYOLOv8PoseBoostAdvancementsMultimodal2024,
  title = {{{YOLOv8-PoseBoost}}: {{Advancements}} in {{Multimodal Robot Pose Keypoint Detection}}},
  shorttitle = {{{YOLOv8-PoseBoost}}},
  author = {Wang, Feng and Wang, Gang and Lu, Baoli},
  year = {2024},
  month = mar,
  journal = {Electronics},
  volume = {13},
  number = {6},
  pages = {1046},
  issn = {2079-9292},
  doi = {10.3390/electronics13061046},
  urldate = {2024-10-21},
  abstract = {In the field of multimodal robotics, achieving comprehensive and accurate perception of the surrounding environment is a highly sought-after objective. However, current methods still have limitations in motion keypoint detection, especially in scenarios involving small target detection and complex scenes. To address these challenges, we propose an innovative approach known as YOLOv8PoseBoost. This method introduces the Channel Attention Module (CBAM) to enhance the network's focus on small targets, thereby increasing sensitivity to small target individuals. Additionally, we employ multiple scale detection heads, enabling the algorithm to comprehensively detect individuals of varying sizes in images. The incorporation of cross-level connectivity channels further enhances the fusion of features between shallow and deep networks, reducing the rate of missed detections for small target individuals. We also introduce a Scale Invariant Intersection over Union (SIoU) redefined bounding box regression localization loss function, which accelerates model training convergence and improves detection accuracy. Through a series of experiments, we validate YOLOv8-PoseBoost's outstanding performance in motion keypoint detection for small targets and complex scenes. This innovative approach provides an effective solution for enhancing the perception and execution capabilities of multimodal robots. It has the potential to drive the development of multimodal robots across various application domains, holding both theoretical and practical significance.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\8JAC355V\Wang 等 - 2024 - YOLOv8-PoseBoost Advancements in Multimodal Robot Pose Keypoint Detection.pdf}
}

@article{weiNewQuasiNewtonMethods2006,
  title = {New Quasi-{{Newton}} Methods for Unconstrained Optimization Problems},
  author = {Wei, Zengxin and Li, Guoyin and Qi, Liqun},
  year = {2006},
  month = apr,
  journal = {Applied Mathematics and Computation},
  volume = {175},
  number = {2},
  pages = {1156--1188},
  issn = {00963003},
  doi = {10.1016/j.amc.2005.08.027},
  urldate = {2025-04-23},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\4846K6W8\\Wei 等 - 2006 - New quasi-Newton methods for unconstrained optimization problems.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\7EP8TWQX\\New_quasi_Newton_methods_for_unconstrain.zh-CN.mono.pdf}
}

@article{WuLinJiYuJiSuanJiShiJueDeGongYeJinShuBiaoMianQueXianJianCeZongShu2024a,
  type = {10.16383/j.Aas.C230039},
  title = {基于计算机视觉的工业金属表面缺陷检测综述},
  author = {伍麟 and 郝鸿宇 and 宋友},
  year = {2024},
  journal = {自动化学报},
  volume = {50},
  number = {07},
  pages = {1261--1283},
  abstract = {针对平面及三维结构金属材料的工业表面缺陷检测,概述了视觉检测技术的基本原理和研究现状,并总结出视觉自动检测系统的关键技术包括光学成像技术、图像预处理技术与缺陷检测器.首先介绍了如何根据检测对象的光学特性选择合适的二维、三维光学成像技术;其次介绍了图像降噪、特征提取、图像分割和拼接等预处理技术的重要作用;然后根据缺陷检测器的实现原理将其分为模板匹配、图像分类、图像语义分割、目标检测和图像异常检测五类,并对其中的经典算法进行了归纳分析.最后,探讨了工业场景下金属表面缺陷检测技术实施中的关键问题,并对该技术的发展趋势进行了展望.},
  isbn = {0254-4156},
  lccn = {11-2109/TP},
  keywords = {;;;}
}

@article{wuReviewMetalSurface2024,
  title = {A Review of Metal Surface Defect Detection Based on Computer Vision},
  author = {Wu, L. and Hao, H. Y. and Song, Y.},
  year = {2024},
  journal = {Acta Autom. Sin},
  volume = {50},
  pages = {1261--1283}
}

@article{wuReviewMetalSurface2024a,
  type = {{{https://doi.org/10.16383/j.aas.c230039}}},
  title = {A {{Review}} of {{Metal Surface Defect Detection Based}} on {{Computer Vision}}},
  author = {WU, Lin and HAO, Hong-Yu and SONG, You},
  year = {2024},
  journal = {Acta Automatica Sinica},
  volume = {50},
  number = {7},
  pages = {1261--1283},
  keywords = {automatic inspection system,computer vision,metal surface defect,Surface defect detection}
}

@misc{wuUnleashingLargeScaleVideo2023,
  title = {Unleashing {{Large-Scale Video Generative Pre-training}} for {{Visual Robot Manipulation}}},
  author = {Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13139},
  eprint = {2312.13139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.13139},
  urldate = {2024-12-03},
  abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a straightforward GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9\% to 94.9\%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3\% to 85.4\%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\2SWRSSQL\Wu 等 - 2023 - Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation.pdf}
}

@article{XieLiangKeDianZiShuXianZhiShiBiaoZaiShuXianBanJingGuiShangDeYingYong2015,
  type = {10.16567/j.Cnki.1000-7008.2015.09.032},
  title = {电子数显指示表在数显半径规上的应用},
  author = {谢两可},
  year = {2015},
  journal = {工具技术},
  volume = {49},
  number = {09},
  pages = {108},
  abstract = {一般情况下都认为数显指示表只能进行长度方向测量,比如应用在机床上的深度检测、测厚规等,其实经过简单的几何换算,指示表还可以用来测量圆弧半径。如图1所示,OA、OB和OC为圆弧半径,其中B位于AC的垂直中心线上,AC小于所测圆弧的直径,根据勾股定理,CD2+OD2=OC2,如果BD可以测量出来,设圆弧半径为R,那么就有如下等式C2+(R-BD)2=R2},
  isbn = {1000-7008},
  lccn = {51-1271/TH},
  file = {C:\Users\guaoxiang\Zotero\storage\4GS6D5BJ\谢两可 - 2015 - 电子数显指示表在数显半径规上的应用.pdf}
}

@misc{xuFASTLIOFastRobust2021,
  title = {{{FAST-LIO}}: {{A Fast}}, {{Robust LiDAR-inertial Odometry Package}} by {{Tightly-Coupled Iterated Kalman Filter}}},
  shorttitle = {{{FAST-LIO}}},
  author = {Xu, Wei and Zhang, Fu},
  year = {2021},
  month = apr,
  number = {arXiv:2010.08196},
  eprint = {2010.08196},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {This paper presents a computationally efficient and robust LiDAR-inertial odometry framework. We fuse LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. To lower the computation load in the presence of large number of measurements, we present a new formula to compute the Kalman gain. The new formula has computation load depending on the state dimension instead of the measurement dimension. The proposed method and its implementation are tested in various indoor and outdoor environments. In all tests, our method produces reliable navigation results in real-time: running on a quadrotor onboard computer, it fuses more than 1,200 effective feature points in a scan and completes all iterations of an iEKF step within 25 ms. Our codes are open-sourced on Github.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3UCVV3L8\\Xu和Zhang - 2021 - FAST-LIO A Fast, Robust LiDAR-inertial Odometry Package by Tightly-Coupled Iterated Kalman Filter.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\MR2J7BLA\\2010.html}
}

@misc{xuPIDNetRealtimeSemantic2023,
  title = {{{PIDNet}}: {{A Real-time Semantic Segmentation Network Inspired}} by {{PID Controllers}}},
  shorttitle = {{{PIDNet}}},
  author = {Xu, Jiacong and Xiong, Zixiang and Bhattacharyya, Shankar P.},
  year = {2023},
  month = apr,
  number = {arXiv:2206.02066},
  eprint = {2206.02066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-IntegralDerivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel threebranch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6\% mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1\% mIOU with speed of 153.7 FPS on CamVid.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\VCSCPDXZ\Xu 等 - 2023 - PIDNet A Real-time Semantic Segmentation Network Inspired by PID Controllers.pdf}
}

@misc{yangCompositionalDiffusionBasedContinuous2023,
  title = {Compositional {{Diffusion-Based Continuous Constraint Solvers}}},
  author = {Yang, Zhutian and Mao, Jiayuan and Du, Yilun and Wu, Jiajun and Tenenbaum, Joshua B. and {Lozano-P{\'e}rez}, Tom{\'a}s and Kaelbling, Leslie Pack},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00966},
  eprint = {2309.00966},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.00966},
  urldate = {2025-08-11},
  abstract = {This paper introduces an approach for learning to solve continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning. Previous methods primarily rely on hand-engineering or learning generators for specific constraint types and then rejecting the value assignments when other constraints are violated. By contrast, our model, the compositional diffusion continuous constraint solver (Diffusion-CCSP) derives global solutions to CCSPs by representing them as factor graphs and combining the energies of diffusion models trained to sample for individual constraint types. Diffusion-CCSP exhibits strong generalization to novel combinations of known constraints, and it can be integrated into a task and motion planner to devise long-horizon plans that include actions with both discrete and continuous parameters. Project site: https://diffusion-ccsp.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\C4728UK3\\Yang 等 - 2023 - Compositional Diffusion-Based Continuous Constraint Solvers.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\FHRXCNXE\\2309.html}
}

@book{YangJuZhenLun2003,
  title = {矩阵论},
  author = {杨, 明 and 刘, 先忠},
  year = {2003},
  edition = {第二版},
  publisher = {华中科技大学出版社},
  file = {C:\Users\guaoxiang\Zotero\storage\ITXWM2XQ\《矩阵论（第二版）》【杨明】.pdf}
}

@article{YinYongKaiTiaoWenTouYingLunKuoShuXiTongMoXingYuBiaoDingZongShu2020,
  title = {条纹投影轮廓术系统模型与标定综述},
  author = {殷永凯 and 张宗华 and 刘晓利 and 彭翔},
  year = {2020},
  journal = {红外与激光工程},
  volume = {49},
  number = {3},
  pages = {0303008--18},
  publisher = {《 红外与激光工程》 编辑部},
  isbn = {1007-2276}
}

@article{yuCuttingPlaneBased,
  title = {Cutting {{Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data}} for {{Digital Fringe Projection}}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  abstract = {Benefiting from the characteristics of full field scanning, high resolution and high precision, digital fringe projection measurement technology has been widely used in three-dimensional measurement. For the cylinder pose detection, due to the influence of occlusion and noise, the obtained three-dimensional point cloud information is usually incomplete, resulting in the unreliable object parameters after point cloud fitting. In view of the above problems, a cylinder fitting method with incomplete point cloud data based on the cutting plane is proposed. Firstly, the point cloud is processed by bilateral filtering method to reduce the influence of outliers and noise points. Then, according to the principle of forming ellipse on cylinder cutting plane, some equidistant cutting planes are selected. At the same time, in order to reduce the influence of outliers on fitting, the random sampling consistency algorithm is used for ellipse fitting with the points on each cutting plane, that is, the ellipse parameters corresponding to each cutting plane can be obtained. Finally, random sampling consistency algorithm is applied for cylinder axis fitting with the ellipse centers of all the cutting planes, calculating the axis vector of the cylinder. Thus, the parameters of the cylinder detected are obtained. The experimental results of stepped cylinder with diffuse reflection and metal cylinder show the effectiveness and high precision of the proposed method.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\92JSH3DZ\Yu 等 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@article{yuCuttingPlaneBased2020,
  title = {Cutting {{Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data}} for {{Digital Fringe Projection}}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {149385--149401},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3016424},
  urldate = {2024-11-25},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\KL9W45ZI\Yu 等 - 2020 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@article{yuCuttingPlaneBased2020a,
  title = {{Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Projection}},
  author = {Yu, Changzhi and Ji, Fang and Xue, Junpeng},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {149385--149401},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3016424},
  urldate = {2024-11-25},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {chinese},
  file = {C:\Users\guaoxiang\Zotero\storage\P6CQBWAT\Yu 等 - 2020 - Cutting Plane Based Cylinder Fitting Method With Incomplete Point Cloud Data for Digital Fringe Proj.pdf}
}

@misc{ze3DDiffusionPolicy2024,
  title = {{3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations}},
  shorttitle = {{3D Diffusion Policy}},
  author = {Ze, Yanjie and Zhang, Gu and Zhang, Kangning and Hu, Chenyuan and Wang, Muhan and Xu, Huazhe},
  year = {2024},
  month = sep,
  number = {arXiv:2403.03954},
  eprint = {2403.03954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03954},
  urldate = {2025-08-24},
  abstract = {Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2\% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85\%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\JMFI3H57\\2507.01925v1.no_watermark.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\WTZHHVX6\\Ze 等 - 2024 - 3D Diffusion Policy Generalizable Visuomotor Policy Learning via Simple 3D Representations.pdf}
}

@misc{zhangGAMMAGraspabilityAwareMobile2024,
  title = {{GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion}},
  shorttitle = {{GAMMA}},
  author = {Zhang, Jiazhao and Gireesh, Nandiraju and Wang, Jilong and Fang, Xiaomeng and Xu, Chaoyi and Chen, Weiguang and Dai, Liu and Wang, He},
  year = {2024},
  month = mar,
  number = {arXiv:2309.15459},
  eprint = {2309.15459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.15459},
  urldate = {2024-09-25},
  abstract = {Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses.},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\LQWSWFIX\\Zhang 等 - 2024 - GAMMA Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\9DUDT2VI\\2309.html}
}

@misc{zhangPointCLIPPointCloud2021,
  title = {{{PointCLIP}}: {{Point Cloud Understanding}} by {{CLIP}}},
  shorttitle = {{{PointCLIP}}},
  author = {Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  year = {2021},
  month = dec,
  number = {arXiv:2112.02413},
  eprint = {2112.02413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.02413},
  urldate = {2025-08-11},
  abstract = {Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing PointCLIP, which conducts alignment between CLIP-encoded point cloud and 3D category texts. Specifically, we encode a point cloud by projecting it into multi-view depth maps without rendering, and aggregate the view-wise zero-shot prediction to achieve knowledge transfer from 2D to 3D. On top of that, we design an inter-view adapter to better extract the global feature and adaptively fuse the few-shot knowledge learned from 3D into CLIP pre-trained in 2D. By just fine-tuning the lightweight adapter in the few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the complementary property between PointCLIP and classical 3D-supervised networks. By simple ensembling, PointCLIP boosts baseline's performance and even surpasses state-of-the-art models. Therefore, PointCLIP is a promising alternative for effective 3D point cloud understanding via CLIP under low resource cost and data regime. We conduct thorough experiments on widely-adopted ModelNet10, ModelNet40 and the challenging ScanObjectNN to demonstrate the effectiveness of PointCLIP. The code is released at https://github.com/ZrrSkywalker/PointCLIP.},
  archiveprefix = {arXiv},
  keywords = {,3D,CLIP,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\XWWHP9P8\\Zhang 等 - 2021 - PointCLIP Point Cloud Understanding by CLIP.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\94T47QZL\\2112.html}
}

@article{zhanImprovingEyeDisplay2019,
  title = {Improving Near-Eye Display Resolution by Polarization Multiplexing},
  author = {Zhan, Tao and Xiong, Jianghao and Tan, Guanjun and Lee, Yun-Han and Yang, Jilin and Liu, Sheng and Wu, Shin-Tson},
  year = {2019},
  month = may,
  journal = {Optics Express},
  volume = {27},
  number = {11},
  pages = {15327},
  issn = {1094-4087},
  doi = {10.1364/OE.27.015327},
  urldate = {2024-10-14},
  abstract = {We present here an optical approach to boost the apparent pixel density by utilizing the superimposition of two shifted-pixel grids generated by a Pancharatnam-Berry deflector (PBD). The content of the two shifted pixel grids are presented to the observer's eye simultaneously using a polarization-multiplexing method. Considering the compact and lightweight nature of PBD, this approach has potential applications in near-eye display systems. Moreover, the same concept can be extended to projection displays with proper modifications.},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\BG8WLZ9Q\Zhan 等 - 2019 - Improving near-eye display resolution by polarization multiplexing.pdf}
}

@misc{zhaoAgentCerebrumController2023,
  title = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}: {{Implementing}} an {{Embodied LMM-based Agent}} on {{Drones}}},
  shorttitle = {Agent as {{Cerebrum}}, {{Controller}} as {{Cerebellum}}},
  author = {Zhao, Haoran and Pan, Fengxing and Ping, Huqiuyue and Zhou, Yaoming},
  year = {2023},
  month = nov,
  number = {arXiv:2311.15033},
  eprint = {2311.15033},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-07},
  abstract = {In this study, we present a novel paradigm for industrial robotic embodied agents, encapsulating an agent as cerebrum, controller as cerebellum architecture. Our approach harnesses the power of Large Multimodal Models (LMMs) within an agent framework known as AeroAgent, tailored for drone technology in industrial settings. To facilitate seamless integration with robotic systems, we introduce ROSchain, a bespoke linkage framework connecting LMM-based agents to the Robot Operating System (ROS). We report findings from extensive empirical research, including simulated experiments on the Airgen and real-world case study, particularly in individual search and rescue operations. The results demonstrate AeroAgent's superior performance in comparison to existing Deep Reinforcement Learning (DRL)-based agents, highlighting the advantages of the embodied LMM in complex, real-world scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\54QUDRMU\Zhao 等 - 2023 - Agent as Cerebrum, Controller as Cerebellum Implementing an Embodied LMM-based Agent on Drones.pdf}
}

@inproceedings{zhaoARassistedHumanRobotInteraction2024,
  title = {An {{AR-assisted Human-Robot Interaction System}} for {{Improving LLM-based Robot Control}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Cybernetics}} and {{Intelligent Systems}} ({{CIS}}) and {{IEEE International Conference}} on {{Robotics}}, {{Automation}} and {{Mechatronics}} ({{RAM}})},
  author = {Zhao, Ziyue and Lou, Shanhe and Tan, Runjia and Lv, Chen},
  year = {2024},
  month = aug,
  pages = {144--149},
  issn = {2326-8239},
  doi = {10.1109/CIS-RAM61939.2024.10673005},
  urldate = {2024-11-18},
  abstract = {The LLM-based robot control system enables robots to understand and execute high-level language instructions. However, the motion plans generated by the LLM-based robot control system are not always reliable. To allow operators to monitor the working state of the robot after issuing task instructions in high-level language, and demonstrate the robot's motion plan safely, we designed an AR-assisted human-robot interaction system called SeeIt. This system displays the working state of a robotic arm and predicts its motion trajectory regarding LLM output. Ambiguities in high-level language instructions and perception misunderstanding about the surrounding environment may lead to erroneous motion plans. The AR interaction system allows operators to choose among potential running trajectories and to adjust erroneous motion plans, leveraging human decision-making to enhance the system's intelligence. A case study was conducted to evaluate the usability and performance of this system. We expected that SeeIt will increase the interaction experience between operator and robot, improving the task performance of the LLM-based robot control system.},
  langid = {american},
  keywords = {argument reality,arm,digital twin,High level languages,human-robot interaction,Human-robot interaction,llm,motion planning,Planning,Random access memory,Reliability engineering,Robot control,Trajectory},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\3HDFA2IX\\Zhao 等 - 2024 - An AR-assisted Human-Robot Interaction System for Improving LLM-based Robot Control.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\UMHF77U6\\10673005.html}
}

@misc{zhaoLearningFineGrainedBimanual2023,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13705},
  eprint = {2304.13705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  urldate = {2024-12-03},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\guaoxiang\Zotero\storage\8RWRW2SS\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf}
}

@misc{zhaoLearningFineGrainedBimanual2023a,
  title = {Learning {{Fine-Grained Bimanual Manipulation}} with {{Low-Cost Hardware}}},
  author = {Zhao, Tony Z. and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13705},
  eprint = {2304.13705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13705},
  urldate = {2025-08-06},
  abstract = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90\% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\2HPAV2AZ\\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.zh-CN.mono.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\GB6LF9ZE\\Zhao 等 - 2023 - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware.pdf}
}

@inproceedings{zhengyouzhangFlexibleCameraCalibration1999,
  title = {Flexible Camera Calibration by Viewing a Plane from Unknown Orientations},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {{Zhengyou Zhang}},
  year = {1999},
  pages = {666-673 vol.1},
  publisher = {IEEE},
  address = {Kerkyra, Greece},
  doi = {10.1109/ICCV.1999.791289},
  urldate = {2024-11-11},
  abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use. The corresponding software is available from the author's Web page.},
  isbn = {978-0-7695-0164-2},
  langid = {english},
  file = {C:\Users\guaoxiang\Zotero\storage\XECWGQE4\Zhengyou Zhang - 1999 - Flexible camera calibration by viewing a plane from unknown orientations.pdf}
}

@misc{zhongSurveyVisionLanguageActionModels2025,
  title = {A {{Survey}} on {{Vision-Language-Action Models}}: {{An Action Tokenization Perspective}}},
  shorttitle = {A {{Survey}} on {{Vision-Language-Action Models}}},
  author = {Zhong, Yifan and Bai, Fengshuo and Cai, Shaofei and Huang, Xuchuan and Chen, Zhang and Zhang, Xiaowei and Wang, Yuanfei and Guo, Shaoyang and Guan, Tianrui and Lui, Ka Nam and Qi, Zhiquan and Liang, Yitao and Chen, Yuanpei and Yang, Yaodong},
  year = {2025},
  month = jul,
  number = {arXiv:2507.01925},
  eprint = {2507.01925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.01925},
  urldate = {2025-09-11},
  abstract = {The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of {\textbackslash}textit\{action tokens\} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\guaoxiang\\Zotero\\storage\\2RPEP5GZ\\Zhong 等 - 2025 - A Survey on Vision-Language-Action Models An Action Tokenization Perspective.pdf;C\:\\Users\\guaoxiang\\Zotero\\storage\\6T44C5EI\\Zhong 等 - 2025 - A Survey on Vision-Language-Action Models An Action Tokenization Perspective.pdf}
}

@misc{zhongSurveyVisionLanguageActionModels2025a,
  title = {{A Survey on Vision-Language-Action Models: An Action Tokenization Perspective}},
  shorttitle = {{A Survey on Vision-Language-Action Models}},
  author = {Zhong, Yifan and Bai, Fengshuo and Cai, Shaofei and Huang, Xuchuan and Chen, Zhang and Zhang, Xiaowei and Wang, Yuanfei and Guo, Shaoyang and Guan, Tianrui and Lui, Ka Nam and Qi, Zhiquan and Liang, Yitao and Chen, Yuanpei and Yang, Yaodong},
  year = {2025},
  month = jul,
  number = {arXiv:2507.01925},
  eprint = {2507.01925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.01925},
  urldate = {2025-09-11},
  abstract = {The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of {\textbackslash}textit\{action tokens\} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence.},
  archiveprefix = {arXiv},
  langid = {chinese},
  keywords = {Computer Science - Robotics}
}

@article{zotero-1199,
  type = {Article}
}

@misc{zouDiffBEVConditionalDiffusion2023,
  title = {{{DiffBEV}}: {{Conditional Diffusion Model}} for {{Bird}}'s {{Eye View Perception}}},
  shorttitle = {{{DiffBEV}}},
  author = {Zou, Jiayu and Zhu, Zheng and Ye, Yun and Wang, Xingang},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08333},
  eprint = {2303.08333},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08333},
  urldate = {2025-08-31},
  abstract = {BEV perception is of great importance in the field of autonomous driving, serving as the cornerstone of planning, controlling, and motion prediction. The quality of the BEV feature highly affects the performance of BEV perception. However, taking the noises in camera parameters and LiDAR scans into consideration, we usually obtain BEV representation with harmful noises. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to utilize the diffusion model to get a better BEV representation. In this work, we propose an endto-end framework, named DiffBEV, to exploit the potential of diffusion model to generate a more comprehensive BEV representation. To the best of our knowledge, we are the first to apply diffusion model to BEV perception. In practice, we design three types of conditions to guide the training of the diffusion model which denoises the coarse samples and refines the semantic feature in a progressive way. What's more, a cross-attention module is leveraged to fuse the context of BEV feature and the semantic content of conditional diffusion model. DiffBEV achieves a 25.9\% mIoU on the nuScenes dataset, which is 6.2\% higher than the bestperforming existing approach. Quantitative and qualitative results on multiple benchmarks demonstrate the effectiveness of DiffBEV in BEV semantic segmentation and 3D object detection tasks. The code {\ddag} will be available soon.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\guaoxiang\Zotero\storage\84KVF3I5\Zou 等 - 2023 - DiffBEV Conditional Diffusion Model for Bird's Eye View Perception.pdf}
}
